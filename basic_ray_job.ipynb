{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1336f048-16eb-441b-9c12-74309c613578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 20:37:35,479\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-03 20:37:35,557\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-03 20:37:35,698\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost as mlflow_xgb\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "from ray.data import read_parquet\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "try:\n",
    "    from ray.tune.callback import Callback      # Ray >= 2.6\n",
    "except ImportError:\n",
    "    from ray.tune.callbacks import Callback     # Older Ray\n",
    "from utils import ddl_cluster_scaling_client\n",
    "from utils import mlflow_utils\n",
    "from utils import ray_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a54bc1-58ed-42ae-ae0c-341a0ca2d442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fe473df-6ff9-4cce-8bbf-10f4493cb6dd",
   "metadata": {},
   "source": [
    "## Pre-requsites\n",
    "\n",
    "Configure the following user environment variables\n",
    "\n",
    "1. AWS_ROLE_ARN - This is the AWS role being assumed via IR\n",
    "2. S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8af9bda-73fe-4dde-b2b5-c9c6e54ee9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../tmp/train.parquet to s3://ddl-wadkars/end-to-end/california/train/train.parquet\n",
      "upload: ../../tmp/val.parquet to s3://ddl-wadkars/end-to-end/california/val/val.parquet\n",
      "upload: ../../tmp/test.parquet to s3://ddl-wadkars/end-to-end/california/test/test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download dataset and push to S3\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.rename(columns={\"MedHouseVal\": \"median_house_value\"})\n",
    "\n",
    "# Split\n",
    "train, tmp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val, test  = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save locally\n",
    "train.to_parquet(\"/tmp/train.parquet\", index=False)\n",
    "val.to_parquet(\"/tmp/val.parquet\", index=False)\n",
    "test.to_parquet(\"/tmp/test.parquet\", index=False)\n",
    "\n",
    "# Push to S3\n",
    "!aws s3 cp /tmp/train.parquet s3://${S3_BUCKET_NAME}/end-to-end/california/train/\n",
    "!aws s3 cp /tmp/val.parquet   s3://${S3_BUCKET_NAME}/end-to-end/california/val/\n",
    "!aws s3 cp /tmp/test.parquet  s3://${S3_BUCKET_NAME}/end-to-end/california/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7040bd1-cd6e-4be0-98d9-2f90d4a527ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1adda697-3841-450e-98f4-bed31db79159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _s3p(root: str, sub: str) -> str:\n",
    "    \"\"\"Safe join for S3/posix URIs.\"\"\"\n",
    "    return f\"{root.rstrip('/')}/{sub.lstrip('/')}\"\n",
    "\n",
    "\n",
    "def read_parquet_to_pandas(uri: str, columns=None, limit: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust Parquetâ†’pandas loader that bypasses Ray Data.\n",
    "    Works with local paths and s3:// (PyArrow uses AWS_* env vars / IRSA).\n",
    "    \"\"\"\n",
    "    ds = pds.dataset(uri.rstrip(\"/\"), format=\"parquet\")\n",
    "    if limit is None:\n",
    "        return ds.to_table(columns=columns).to_pandas()\n",
    "\n",
    "    # Respect limit across files/row groups\n",
    "    scanner = pds.Scanner.from_dataset(ds, columns=columns)\n",
    "    batches, rows = [], 0\n",
    "    for b in scanner.to_batches():\n",
    "        batches.append(b)\n",
    "        rows += len(b)\n",
    "        if rows >= limit:\n",
    "            return pa.Table.from_batches(batches)[:limit].to_pandas()\n",
    "    return pa.Table.from_batches(batches).to_pandas()\n",
    "\n",
    "\n",
    "def main(experiment_name:str,data_dir: str,\n",
    "         model_name:str,model_desc:str,\n",
    "         num_workers: int = 4, cpus_per_worker: int = 1,  DEV_FAST: bool = False):\n",
    "    \"\"\"\n",
    "    Quick knobs:\n",
    "      - num_workers * cpus_per_worker = CPUs per trial.\n",
    "      - trainer_resources={\"CPU\":0} so the driver doesn't steal a core.\n",
    "      - PACK placement to keep trials tight.\n",
    "      - max_concurrent_trials caps parallel trials.\n",
    "      - num_boost_round / early_stopping_rounds control trial length.\n",
    "      - nthread = cpus_per_worker to avoid oversubscription.\n",
    "    \"\"\"\n",
    "\n",
    "    exp_id = mlflow_utils.ensure_mlflow_experiment(experiment_name)\n",
    "    exp_id = mlflow_utils.ensure_registered_model(model_name)\n",
    "    # Storage: local for dev, S3/your env otherwise\n",
    "    RUN_STORAGE = os.environ.get(\"RAY_AIR_STORAGE\", f\"{data_dir}/air/xgb\")\n",
    "    TUNER_STORAGE = \"/tmp/air-dev\" if DEV_FAST else RUN_STORAGE\n",
    "    FINAL_STORAGE = \"/mnt/data/ddl-end-to-end-demo/air/final_fit\" if DEV_FAST else RUN_STORAGE\n",
    "\n",
    "    # Sanity: workers see IRSA env?\n",
    "    @ray.remote\n",
    "    def _peek():\n",
    "        import os\n",
    "        return {\n",
    "            \"ROLE\": bool(os.environ.get(\"AWS_ROLE_ARN\")),\n",
    "            \"TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\"),\n",
    "            \"REGION\": os.environ.get(\"AWS_REGION\"),\n",
    "        }\n",
    "    print(\"Worker env peek:\", ray.get(_peek.remote()))\n",
    "\n",
    "    # MLflow (experiment + parent run)\n",
    "    CLUSTER_TRACKING_URI = os.environ[\"CLUSTER_MLFLOW_TRACKING_URI\"]\n",
    "    \n",
    "    \n",
    "    client = MlflowClient()\n",
    "\n",
    "\n",
    "    parent = client.create_run(\n",
    "        experiment_id=exp_id,\n",
    "        tags={\"mlflow.runName\": \"xgb_parent\", \"role\": \"tune_parent\"},\n",
    "    )\n",
    "    parent_run_id = parent.info.run_id\n",
    "    print(\"Parent run id:\", parent_run_id)\n",
    "\n",
    "    # Data (Ray Datasets for training/val)\n",
    "    train_ds = read_parquet(_s3p(data_dir, \"train\"), parallelism=num_workers)\n",
    "    val_ds   = read_parquet(_s3p(data_dir, \"val\"),   parallelism=num_workers)\n",
    "    test_ds  = read_parquet(_s3p(data_dir, \"test\"),  parallelism=num_workers)\n",
    "    print(\"Schema:\", train_ds.schema())\n",
    "\n",
    "    # Label + features\n",
    "    label_col = \"median_house_value\"\n",
    "    feature_cols = [c for c in train_ds.schema().names if c != label_col]\n",
    "    keep = feature_cols + [label_col]\n",
    "    train_ds = train_ds.select_columns(keep)\n",
    "    val_ds   = val_ds.select_columns(keep)\n",
    "\n",
    "    # DEV: trim Ray Datasets used for training; eval will bypass Ray entirely\n",
    "    if DEV_FAST:\n",
    "        train_ds = train_ds.limit(5_000)\n",
    "        val_ds   = val_ds.limit(2_000)\n",
    "\n",
    "    # --- Build test DataFrame without Ray (avoids 'Global node is not initialized') ---\n",
    "    test_uri = _s3p(data_dir, \"test\")\n",
    "    test_pdf = read_parquet_to_pandas(\n",
    "        test_uri, columns=keep, limit=2_000 if DEV_FAST else None\n",
    "    )\n",
    "\n",
    "    # Search space\n",
    "    param_space = {\n",
    "        \"params\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": tune.loguniform(1e-3, 3e-1),\n",
    "            \"max_depth\": tune.randint(4, 12),\n",
    "            \"min_child_weight\": tune.loguniform(1e-2, 10),\n",
    "            \"subsample\": tune.uniform(0.6, 1.0),\n",
    "            \"colsample_bytree\": tune.uniform(0.6, 1.0),\n",
    "            \"lambda\": tune.loguniform(1e-3, 10),\n",
    "            \"alpha\": tune.loguniform(1e-3, 10),\n",
    "        },\n",
    "        \"num_boost_round\": 300,\n",
    "        \"early_stopping_rounds\": 20,\n",
    "    }\n",
    "\n",
    "    # Dev shortcuts\n",
    "    if DEV_FAST:\n",
    "        param_space[\"num_boost_round\"] = 20\n",
    "        param_space[\"early_stopping_rounds\"] = 5\n",
    "        num_workers = 1\n",
    "        cpus_per_worker = 1\n",
    "        NUM_SAMPLES = 5\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "    else:\n",
    "        NUM_SAMPLES = 30\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "\n",
    "    # Threads per worker\n",
    "    param_space[\"params\"][\"nthread\"] = cpus_per_worker\n",
    "    print(\"Per-trial CPUs =\", num_workers * cpus_per_worker)\n",
    "\n",
    "    # Scaling / placement\n",
    "    scaling = ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=False,\n",
    "        resources_per_worker={\"CPU\": cpus_per_worker},\n",
    "        trainer_resources={\"CPU\": 0},\n",
    "        placement_strategy=\"PACK\",\n",
    "    )\n",
    "\n",
    "    # Trainable\n",
    "    trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=param_space[\"params\"],\n",
    "        datasets={\"train\": train_ds, \"valid\": val_ds},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "    )\n",
    "\n",
    "    # Search + scheduler\n",
    "    MAX_T = int(param_space[\"num_boost_round\"])\n",
    "    GRACE = int(min(param_space.get(\"early_stopping_rounds\", 1), MAX_T))\n",
    "    algo = HyperOptSearch(metric=\"valid-rmse\", mode=\"min\")\n",
    "    scheduler = ASHAScheduler(max_t=MAX_T, grace_period=GRACE, reduction_factor=3)\n",
    "\n",
    "    # MLflow callback (child runs)\n",
    "    mlflow_cb = MLflowLoggerCallback(\n",
    "        tracking_uri=CLUSTER_TRACKING_URI,\n",
    "        experiment_name=experiment_name,\n",
    "        save_artifact=SAVE_ARTIFACTS,\n",
    "        tags={\"mlflow.parentRunId\": parent_run_id},\n",
    "    )\n",
    "\n",
    "    # Tuner\n",
    "    tuner = tune.Tuner(\n",
    "        trainer.as_trainable(),\n",
    "        run_config=RunConfig(\n",
    "            name=\"xgb_from_s3_irsa\",\n",
    "            storage_path=TUNER_STORAGE,\n",
    "            callbacks=[mlflow_cb],\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            metric=\"valid-rmse\",\n",
    "            mode=\"min\",\n",
    "            num_samples=NUM_SAMPLES,\n",
    "            max_concurrent_trials=MAX_CONCURRENT,\n",
    "        ),\n",
    "        param_space={\"params\": param_space[\"params\"]},\n",
    "    )\n",
    "\n",
    "    # Tune\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"valid-rmse\", mode=\"min\")\n",
    "    print(\"Best config:\", best.config)\n",
    "    print(\"Best valid RMSE:\", best.metrics.get(\"valid-rmse\"))\n",
    "\n",
    "    # Final fit (train + val)\n",
    "    merged = train_ds.union(val_ds)\n",
    "    final_trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=best.config[\"params\"],\n",
    "        datasets={\"train\": merged},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "        run_config=RunConfig(name=\"final_fit\", storage_path=FINAL_STORAGE),\n",
    "    )\n",
    "    final_result = final_trainer.fit()\n",
    "    final_ckpt = final_result.checkpoint\n",
    "\n",
    "    # Load Booster from checkpoint\n",
    "    with final_ckpt.as_directory() as ckpt_dir:\n",
    "        print(\"Checkpoint dir:\", ckpt_dir, \"files:\", os.listdir(ckpt_dir))\n",
    "        candidates = [\"model.json\", \"model.ubj\", \"model.xgb\", \"xgboost_model.json\", \"model\"]\n",
    "        model_path = next(\n",
    "            (os.path.join(ckpt_dir, f) for f in candidates if os.path.exists(os.path.join(ckpt_dir, f))),\n",
    "            None,\n",
    "        )\n",
    "        if not model_path:\n",
    "            raise FileNotFoundError(f\"No XGBoost model file found in checkpoint dir: {ckpt_dir}\")\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(model_path)\n",
    "\n",
    "    # Driver-side eval (no Ray dependency)\n",
    "    X_test = test_pdf.drop(columns=[label_col])\n",
    "    \n",
    "    dmat = xgb.DMatrix(X_test)\n",
    "    y_pred = booster.predict(dmat)\n",
    "    rmse = math.sqrt(((test_pdf[label_col].to_numpy() - y_pred) ** 2).mean())\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    \n",
    "    # Log final under parent\n",
    "\n",
    "    with mlflow.start_run(run_id=parent_run_id):\n",
    "        X_example = X_test.head(5).copy()  \n",
    "        y_example = booster.predict(xgb.DMatrix(X_example))\n",
    "        sig = infer_signature(X_example, y_example)\n",
    "        with mlflow.start_run(run_name=\"final_fit\", nested=True) as final_run:\n",
    "            mlflow.log_params(best.config.get(\"params\", {}))\n",
    "            mlflow.log_dict({\"label\": label_col, \"features\": feature_cols}, \"features.json\")\n",
    "            mlflow.log_metric(\"valid_rmse_best\", float(best.metrics.get(\"valid-rmse\")))\n",
    "            mlflow.log_metric(\"test_rmse\", float(rmse))\n",
    "            model_info = mlflow_xgb.log_model(booster, artifact_path=\"model\",signature=sig,input_example=X_example)\n",
    "\n",
    "            mv = mlflow_utils.register_model_version(model_name=model_name,model_desc=model_desc,\n",
    "                                                model_info=model_info,run=final_run)\n",
    "            \n",
    "            print(\"Name: {}\".format(mv.name))\n",
    "            print(\"Version: {}\".format(mv.version))\n",
    "            print(\"Description: {}\".format(mv.description))\n",
    "            print(\"Status: {}\".format(mv.status))\n",
    "            print(\"Stage: {}\".format(mv.current_stage))\n",
    "            \n",
    "    \n",
    "    run = client.get_run(parent_run_id)\n",
    "    if run.info.status == \"RUNNING\":\n",
    "        client.set_terminated(parent_run_id, \"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaca126-a443-402f-8b44-1c5f9a3f2447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ca34d-56b3-42b4-9ced-c185bb3a7ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb46094-7068-418f-817e-95d7d3053c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee926e-ca00-486d-af2c-53cd603c4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_cluster_scaling_client.scale_cluster(cluster_kind=\"rayclusters\",worker_hw_tier_name=\"Medium\", replicas=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55bb35-622c-48cc-8204-fc219791c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_cluster_scaling_client.wait_until_scaling_complete(cluster_kind=\"rayclusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab02e5-1bff-4223-a424-8a1a78496bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ddl_cluster_scaling_client\n",
    "j = ddl_cluster_scaling_client.restart_head_node(cluster_kind=\"rayclusters\")\n",
    "restarts_at = j['started_at']\n",
    "print(restarts_at)\n",
    "ddl_cluster_scaling_client.restart_head_node_status(cluster_kind=\"rayclusters\",restarted_since=restarts_at)\n",
    "ddl_cluster_scaling_client.wait_until_node_restarted(cluster_kind=\"rayclusters\",restarted_since=restarts_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac0c61b-69d8-4686-858b-67ad14a2d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6320/514924184.py:5: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"conf\"):\n"
     ]
    }
   ],
   "source": [
    "## Read Conf from Hydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "# Point Hydra to your conf/ directory\n",
    "with initialize(config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config\", overrides=[\"env=dev\"])\n",
    "    #print(f\"Running in {cfg.env} environment\")\n",
    "    #print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "    \n",
    "    app_name = cfg.app.name\n",
    "    data_dir = cfg.app.data_dir\n",
    "    experiment_name = cfg.mlflow.experiment_name    \n",
    "    model_name = cfg.mlflow.model_name    \n",
    "    model_desc = cfg.mlflow.model_desc\n",
    "    ray_workers = cfg.env.ray.num_workers\n",
    "    cpus_per_worker = cfg.env.ray.cpus_per_worker\n",
    "    dev_fast = cfg.env.ray.dev_fast\n",
    "    #print(ray_workers)\n",
    "    #print(dev_fast)\n",
    "    \n",
    "# Disable tensorboard integration\n",
    "os.environ[\"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "224ba8ff-d2f0-4ad2-83b8-501b4dc41640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 21:15:11,719\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: ignore_reinit_error, log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker env peek: {'ROLE': True, 'TOKEN_FILE': '/var/run/secrets/eks.amazonaws.com/serviceaccount/token', 'REGION': 'us-west-2'}\n"
     ]
    },
    {
     "ename": "RestException",
     "evalue": "BAD_REQUEST: (psycopg2.errors.InvalidTextRepresentation) invalid input syntax for type integer: \"<RegisteredModel: aliases={}, creation_timestamp=1756934112777, description='', last_updated_timestamp=1756934112777, latest_versions=[], name='ray-xgboost-dev-wadkars', tags={'mlflow.domino.dataset_info': '68b08bc1dc77613e66e63652-68b08bc1dc77613e66e63651',\n 'mlflow.domino.environment_id': '68b0bb70dc77613e66e6368d',\n 'mlflow.domino.environment_revision_id': '68b5a4ebb7910b03aaf26b12',\n 'mlflow.domino.hardware_tier': 'medium-k8s',\n 'mlflow.domino.project_id': '68b08bbfdc77613e66e6364e',\n 'mlflow.domino.project_name': 'ddl-end-to-end-demo',\n 'mlflow.domino.run_id': '68b8950637cefe4fe9452448',\n 'mlflow.domino.run_number': '37',\n 'mlflow.domino.user': 'wadkars',\n 'mlflow.domino.user_id': '68ac9b59ea374e2b81c48d75',\n 'mlflow.source.type': 'NOTEBOOK',\n 'mlflow.user': 'wadkars'}>\"\nLINE 3: WHERE experiments.experiment_id = '<RegisteredModel: aliases...\n                                          ^\n\n[SQL: SELECT experiments.experiment_id AS experiments_experiment_id, experiments.name AS experiments_name, experiments.artifact_location AS experiments_artifact_location, experiments.lifecycle_stage AS experiments_lifecycle_stage, experiments.creation_time AS experiments_creation_time, experiments.last_update_time AS experiments_last_update_time \nFROM experiments \nWHERE experiments.experiment_id = %(experiment_id_1)s AND experiments.lifecycle_stage IN (%(lifecycle_stage_1_1)s, %(lifecycle_stage_1_2)s)]\n[parameters: {'experiment_id_1': \"<RegisteredModel: aliases={}, creation_timestamp=1756934112777, description='', last_updated_timestamp=1756934112777, latest_versions=[], name='ray-x ... (498 characters truncated) ... flow.domino.user': 'wadkars',\\n 'mlflow.domino.user_id': '68ac9b59ea374e2b81c48d75',\\n 'mlflow.source.type': 'NOTEBOOK',\\n 'mlflow.user': 'wadkars'}>\", 'lifecycle_stage_1_1': 'active', 'lifecycle_stage_1_2': 'deleted'}]\n(Background on this error at: https://sqlalche.me/e/20/9h9h)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m ray\u001b[38;5;241m.\u001b[39mshutdown()\n\u001b[1;32m     12\u001b[0m ray_utils\u001b[38;5;241m.\u001b[39mensure_ray_connected(RAY_JOB_ENV,ray_ns\u001b[38;5;241m=\u001b[39mapp_name)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpus_per_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mDEV_FAST\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_fast\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 64\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(experiment_name, data_dir, model_name, model_desc, num_workers, cpus_per_worker, DEV_FAST)\u001b[0m\n\u001b[1;32m     58\u001b[0m CLUSTER_TRACKING_URI \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLUSTER_MLFLOW_TRACKING_URI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     61\u001b[0m client \u001b[38;5;241m=\u001b[39m MlflowClient()\n\u001b[0;32m---> 64\u001b[0m parent \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlflow.runName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxgb_parent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtune_parent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m parent_run_id \u001b[38;5;241m=\u001b[39m parent\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParent run id:\u001b[39m\u001b[38;5;124m\"\u001b[39m, parent_run_id)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mlflow/tracking/client.py:393\u001b[0m, in \u001b[0;36mMlflowClient.create_run\u001b[0;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_run\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     experiment_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     run_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Run:\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03m    Create a :py:class:`mlflow.entities.Run` object that can be associated with\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    metrics, parameters, artifacts, etc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m        status: RUNNING\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:169\u001b[0m, in \u001b[0;36mTrackingServiceClient.create_run\u001b[0;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Extract user from tags\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# This logic is temporary; the user_id attribute of runs is deprecated and will be removed\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# in a later release.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m user_id \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mget(MLFLOW_USER, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_current_time_millis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mRunTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:212\u001b[0m, in \u001b[0;36mRestStore.create_run\u001b[0;34m(self, experiment_id, user_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    202\u001b[0m tag_protos \u001b[38;5;241m=\u001b[39m [tag\u001b[38;5;241m.\u001b[39mto_proto() \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags]\n\u001b[1;32m    203\u001b[0m req_body \u001b[38;5;241m=\u001b[39m message_to_json(\n\u001b[1;32m    204\u001b[0m     CreateRun(\n\u001b[1;32m    205\u001b[0m         experiment_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(experiment_id),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    211\u001b[0m )\n\u001b[0;32m--> 212\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCreateRun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Run\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mrun)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     80\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     81\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:370\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    367\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[1;32m    368\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n\u001b[0;32m--> 370\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m response_to_parse \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    372\u001b[0m js_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_to_parse)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:240\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _can_parse_as_json_object(response\u001b[38;5;241m.\u001b[39mtext):\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RestException(json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext))\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         base_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != 200\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m         )\n",
      "\u001b[0;31mRestException\u001b[0m: BAD_REQUEST: (psycopg2.errors.InvalidTextRepresentation) invalid input syntax for type integer: \"<RegisteredModel: aliases={}, creation_timestamp=1756934112777, description='', last_updated_timestamp=1756934112777, latest_versions=[], name='ray-xgboost-dev-wadkars', tags={'mlflow.domino.dataset_info': '68b08bc1dc77613e66e63652-68b08bc1dc77613e66e63651',\n 'mlflow.domino.environment_id': '68b0bb70dc77613e66e6368d',\n 'mlflow.domino.environment_revision_id': '68b5a4ebb7910b03aaf26b12',\n 'mlflow.domino.hardware_tier': 'medium-k8s',\n 'mlflow.domino.project_id': '68b08bbfdc77613e66e6364e',\n 'mlflow.domino.project_name': 'ddl-end-to-end-demo',\n 'mlflow.domino.run_id': '68b8950637cefe4fe9452448',\n 'mlflow.domino.run_number': '37',\n 'mlflow.domino.user': 'wadkars',\n 'mlflow.domino.user_id': '68ac9b59ea374e2b81c48d75',\n 'mlflow.source.type': 'NOTEBOOK',\n 'mlflow.user': 'wadkars'}>\"\nLINE 3: WHERE experiments.experiment_id = '<RegisteredModel: aliases...\n                                          ^\n\n[SQL: SELECT experiments.experiment_id AS experiments_experiment_id, experiments.name AS experiments_name, experiments.artifact_location AS experiments_artifact_location, experiments.lifecycle_stage AS experiments_lifecycle_stage, experiments.creation_time AS experiments_creation_time, experiments.last_update_time AS experiments_last_update_time \nFROM experiments \nWHERE experiments.experiment_id = %(experiment_id_1)s AND experiments.lifecycle_stage IN (%(lifecycle_stage_1_1)s, %(lifecycle_stage_1_2)s)]\n[parameters: {'experiment_id_1': \"<RegisteredModel: aliases={}, creation_timestamp=1756934112777, description='', last_updated_timestamp=1756934112777, latest_versions=[], name='ray-x ... (498 characters truncated) ... flow.domino.user': 'wadkars',\\n 'mlflow.domino.user_id': '68ac9b59ea374e2b81c48d75',\\n 'mlflow.source.type': 'NOTEBOOK',\\n 'mlflow.user': 'wadkars'}>\", 'lifecycle_stage_1_1': 'active', 'lifecycle_stage_1_2': 'deleted'}]\n(Background on this error at: https://sqlalche.me/e/20/9h9h)"
     ]
    }
   ],
   "source": [
    "\n",
    "RAY_JOB_ENV = {\n",
    "    \"AWS_ROLE_ARN\": os.environ.get(\"AWS_ROLE_ARN\", \"\"),\n",
    "    \"AWS_WEB_IDENTITY_TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\", \"\"),\n",
    "    \"AWS_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"AWS_DEFAULT_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\":\"1\",\n",
    "    \"TUNE_RESULT_BUFFER_LENGTH\": \"16\",\n",
    "    \"TUNE_RESULT_BUFFER_FLUSH_INTERVAL_S\": \"3\",    \n",
    "    \n",
    "}\n",
    "ray.shutdown()\n",
    "ray_utils.ensure_ray_connected(RAY_JOB_ENV,ray_ns=app_name)\n",
    "\n",
    "main(experiment_name=experiment_name,data_dir=data_dir, \n",
    "     model_name=model_name,model_desc=model_desc,\n",
    "     num_workers=4, cpus_per_worker=1,DEV_FAST=dev_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6275c8-9ca7-4900-ab60-de5a420e82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ddl_cluster_scaling_client\n",
    "ddl_cluster_scaling_client.scale_cluster(cluster_kind=\"rayclusters\",worker_hw_tier_id=\"small-k8s\",replicas=1)\n",
    "ddl_cluster_scaling_client.wait_until_scaling_complete(cluster_kind=\"rayclusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e00d6-0f97-4180-a994-7cb7fc0e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, sys, pyarrow as pa, pandas as pd\n",
    "print(\"DRIVER:\", sys.version)\n",
    "print(\"DRIVER pyarrow:\", pa.__version__)\n",
    "print(\"DRIVER pandas :\", pd.__version__)\n",
    "\n",
    "@ray.remote\n",
    "def _env_probe():\n",
    "    import sys, pyarrow as pa, pandas as pd\n",
    "    return {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"pyarrow\": pa.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "    }\n",
    "\n",
    "print(\"WORKER:\", ray.get(_env_probe.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a23211-b6aa-469e-878f-b19ec0943134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd3a8b-6669-45fa-9368-dc722d74746c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
