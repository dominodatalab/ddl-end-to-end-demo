{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1336f048-16eb-441b-9c12-74309c613578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 19:02:49,785\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-08-29 19:02:49,925\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "import mlflow.xgboost as mlflow_xgb\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee47f96-3ab0-4cd0-aee1-7be47c073dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\"] = \"1\"\n",
    "EXPERIMENT_NAME = \"ray-xgb-\" + os.environ.get(\"DOMINO_STARTING_USERNAME\")\n",
    "_exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if _exp is None:\n",
    "    EXPERIMENT_ID = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    EXPERIMENT_ID = _exp.experiment_id\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af9bda-73fe-4dde-b2b5-c9c6e54ee9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.rename(columns={\"MedHouseVal\": \"median_house_value\"})\n",
    "\n",
    "# Split\n",
    "train, tmp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val, test  = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save locally\n",
    "train.to_parquet(\"/tmp/train.parquet\", index=False)\n",
    "val.to_parquet(\"/tmp/val.parquet\", index=False)\n",
    "test.to_parquet(\"/tmp/test.parquet\", index=False)\n",
    "\n",
    "# Push to S3\n",
    "#!aws s3 cp /tmp/train.parquet s3://ADD_BUCKET/navy/california/train/\n",
    "#!aws s3 cp /tmp/val.parquet   s3://ADD_BUCKET/navy/california/val/\n",
    "#!aws s3 cp /tmp/test.parquet  s3://ADD_BUCKET/navy/california/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c056b-bb40-4f25-a415-af89f5f70931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "from ray.data import read_parquet\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "try:\n",
    "    from ray.tune.callback import Callback      # Ray >= 2.6\n",
    "except ImportError:\n",
    "    from ray.tune.callbacks import Callback     # Older Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1adda697-3841-450e-98f4-bed31db79159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.xgboost as mlflow_xgb\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "from ray.data import read_parquet\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "import xgboost as xgb\n",
    "import pyarrow.dataset as pds\n",
    "import pyarrow as pa\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "def _s3p(root: str, sub: str) -> str:\n",
    "    \"\"\"Safe join for S3/posix URIs.\"\"\"\n",
    "    return f\"{root.rstrip('/')}/{sub.lstrip('/')}\"\n",
    "\n",
    "\n",
    "def read_parquet_to_pandas(uri: str, columns=None, limit: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust Parquetâ†’pandas loader that bypasses Ray Data.\n",
    "    Works with local paths and s3:// (PyArrow uses AWS_* env vars / IRSA).\n",
    "    \"\"\"\n",
    "    ds = pds.dataset(uri.rstrip(\"/\"), format=\"parquet\")\n",
    "    if limit is None:\n",
    "        return ds.to_table(columns=columns).to_pandas()\n",
    "\n",
    "    # Respect limit across files/row groups\n",
    "    scanner = pds.Scanner.from_dataset(ds, columns=columns)\n",
    "    batches, rows = [], 0\n",
    "    for b in scanner.to_batches():\n",
    "        batches.append(b)\n",
    "        rows += len(b)\n",
    "        if rows >= limit:\n",
    "            return pa.Table.from_batches(batches)[:limit].to_pandas()\n",
    "    return pa.Table.from_batches(batches).to_pandas()\n",
    "\n",
    "\n",
    "def main(data_dir: str, num_workers: int = 4, cpus_per_worker: int = 1, DEV_FAST: bool = False):\n",
    "    \"\"\"\n",
    "    Quick knobs:\n",
    "      - num_workers * cpus_per_worker = CPUs per trial.\n",
    "      - trainer_resources={\"CPU\":0} so the driver doesn't steal a core.\n",
    "      - PACK placement to keep trials tight.\n",
    "      - max_concurrent_trials caps parallel trials.\n",
    "      - num_boost_round / early_stopping_rounds control trial length.\n",
    "      - nthread = cpus_per_worker to avoid oversubscription.\n",
    "    \"\"\"\n",
    "\n",
    "    # Storage: local for dev, S3/your env otherwise\n",
    "    RUN_STORAGE = os.environ.get(\"RAY_AIR_STORAGE\", \"s3://ddl-wadkars/navy/air/xgb\")\n",
    "    TUNER_STORAGE = \"/tmp/air-dev\" if DEV_FAST else RUN_STORAGE\n",
    "    FINAL_STORAGE = \"/mnt/data/ddl-end-to-end-demo/air/final_fit\" if DEV_FAST else RUN_STORAGE\n",
    "\n",
    "    # Sanity: workers see IRSA env?\n",
    "    @ray.remote\n",
    "    def _peek():\n",
    "        import os\n",
    "        return {\n",
    "            \"ROLE\": bool(os.environ.get(\"AWS_ROLE_ARN\")),\n",
    "            \"TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\"),\n",
    "            \"REGION\": os.environ.get(\"AWS_REGION\"),\n",
    "        }\n",
    "    print(\"Worker env peek:\", ray.get(_peek.remote()))\n",
    "\n",
    "    # MLflow (experiment + parent run)\n",
    "    CLUSTER_TRACKING_URI = os.environ[\"CLUSTER_MLFLOW_TRACKING_URI\"]\n",
    "    \n",
    "    \n",
    "    client = MlflowClient()\n",
    "    exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    EXPERIMENT_ID = exp.experiment_id \n",
    "\n",
    "    parent = client.create_run(\n",
    "        experiment_id=EXPERIMENT_ID,\n",
    "        tags={\"mlflow.runName\": \"xgb_parent\", \"role\": \"tune_parent\"},\n",
    "    )\n",
    "    PARENT_RUN_ID = parent.info.run_id\n",
    "    print(\"Parent run id:\", PARENT_RUN_ID)\n",
    "\n",
    "    # Data (Ray Datasets for training/val)\n",
    "    train_ds = read_parquet(_s3p(data_dir, \"train\"), parallelism=num_workers)\n",
    "    val_ds   = read_parquet(_s3p(data_dir, \"val\"),   parallelism=num_workers)\n",
    "    test_ds  = read_parquet(_s3p(data_dir, \"test\"),  parallelism=num_workers)\n",
    "    print(\"Schema:\", train_ds.schema())\n",
    "\n",
    "    # Label + features\n",
    "    label_col = \"median_house_value\"\n",
    "    feature_cols = [c for c in train_ds.schema().names if c != label_col]\n",
    "    keep = feature_cols + [label_col]\n",
    "    train_ds = train_ds.select_columns(keep)\n",
    "    val_ds   = val_ds.select_columns(keep)\n",
    "\n",
    "    # DEV: trim Ray Datasets used for training; eval will bypass Ray entirely\n",
    "    if DEV_FAST:\n",
    "        train_ds = train_ds.limit(5_000)\n",
    "        val_ds   = val_ds.limit(2_000)\n",
    "\n",
    "    # --- Build test DataFrame without Ray (avoids 'Global node is not initialized') ---\n",
    "    test_uri = _s3p(data_dir, \"test\")\n",
    "    test_pdf = read_parquet_to_pandas(\n",
    "        test_uri, columns=keep, limit=2_000 if DEV_FAST else None\n",
    "    )\n",
    "\n",
    "    # Search space\n",
    "    param_space = {\n",
    "        \"params\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": tune.loguniform(1e-3, 3e-1),\n",
    "            \"max_depth\": tune.randint(4, 12),\n",
    "            \"min_child_weight\": tune.loguniform(1e-2, 10),\n",
    "            \"subsample\": tune.uniform(0.6, 1.0),\n",
    "            \"colsample_bytree\": tune.uniform(0.6, 1.0),\n",
    "            \"lambda\": tune.loguniform(1e-3, 10),\n",
    "            \"alpha\": tune.loguniform(1e-3, 10),\n",
    "        },\n",
    "        \"num_boost_round\": 300,\n",
    "        \"early_stopping_rounds\": 20,\n",
    "    }\n",
    "\n",
    "    # Dev shortcuts\n",
    "    if DEV_FAST:\n",
    "        param_space[\"num_boost_round\"] = 20\n",
    "        param_space[\"early_stopping_rounds\"] = 5\n",
    "        num_workers = 1\n",
    "        cpus_per_worker = 1\n",
    "        NUM_SAMPLES = 5\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "    else:\n",
    "        NUM_SAMPLES = 30\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "\n",
    "    # Threads per worker\n",
    "    param_space[\"params\"][\"nthread\"] = cpus_per_worker\n",
    "    print(\"Per-trial CPUs =\", num_workers * cpus_per_worker)\n",
    "\n",
    "    # Scaling / placement\n",
    "    scaling = ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=False,\n",
    "        resources_per_worker={\"CPU\": cpus_per_worker},\n",
    "        trainer_resources={\"CPU\": 0},\n",
    "        placement_strategy=\"PACK\",\n",
    "    )\n",
    "\n",
    "    # Trainable\n",
    "    trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=param_space[\"params\"],\n",
    "        datasets={\"train\": train_ds, \"valid\": val_ds},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "    )\n",
    "\n",
    "    # Search + scheduler\n",
    "    MAX_T = int(param_space[\"num_boost_round\"])\n",
    "    GRACE = int(min(param_space.get(\"early_stopping_rounds\", 1), MAX_T))\n",
    "    algo = HyperOptSearch(metric=\"valid-rmse\", mode=\"min\")\n",
    "    scheduler = ASHAScheduler(max_t=MAX_T, grace_period=GRACE, reduction_factor=3)\n",
    "\n",
    "    # MLflow callback (child runs)\n",
    "    mlflow_cb = MLflowLoggerCallback(\n",
    "        tracking_uri=CLUSTER_TRACKING_URI,\n",
    "        experiment_name=EXPERIMENT_NAME,\n",
    "        save_artifact=SAVE_ARTIFACTS,\n",
    "        tags={\"mlflow.parentRunId\": PARENT_RUN_ID},\n",
    "    )\n",
    "\n",
    "    # Tuner\n",
    "    tuner = tune.Tuner(\n",
    "        trainer.as_trainable(),\n",
    "        run_config=RunConfig(\n",
    "            name=\"xgb_from_s3_irsa\",\n",
    "            storage_path=TUNER_STORAGE,\n",
    "            callbacks=[mlflow_cb],\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            metric=\"valid-rmse\",\n",
    "            mode=\"min\",\n",
    "            num_samples=NUM_SAMPLES,\n",
    "            max_concurrent_trials=MAX_CONCURRENT,\n",
    "        ),\n",
    "        param_space={\"params\": param_space[\"params\"]},\n",
    "    )\n",
    "\n",
    "    # Tune\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"valid-rmse\", mode=\"min\")\n",
    "    print(\"Best config:\", best.config)\n",
    "    print(\"Best valid RMSE:\", best.metrics.get(\"valid-rmse\"))\n",
    "\n",
    "    # Final fit (train + val)\n",
    "    merged = train_ds.union(val_ds)\n",
    "    final_trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=best.config[\"params\"],\n",
    "        datasets={\"train\": merged},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "        run_config=RunConfig(name=\"final_fit\", storage_path=FINAL_STORAGE),\n",
    "    )\n",
    "    final_result = final_trainer.fit()\n",
    "    final_ckpt = final_result.checkpoint\n",
    "\n",
    "    # Load Booster from checkpoint\n",
    "    with final_ckpt.as_directory() as ckpt_dir:\n",
    "        print(\"Checkpoint dir:\", ckpt_dir, \"files:\", os.listdir(ckpt_dir))\n",
    "        candidates = [\"model.json\", \"model.ubj\", \"model.xgb\", \"xgboost_model.json\", \"model\"]\n",
    "        model_path = next(\n",
    "            (os.path.join(ckpt_dir, f) for f in candidates if os.path.exists(os.path.join(ckpt_dir, f))),\n",
    "            None,\n",
    "        )\n",
    "        if not model_path:\n",
    "            raise FileNotFoundError(f\"No XGBoost model file found in checkpoint dir: {ckpt_dir}\")\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(model_path)\n",
    "\n",
    "    # Driver-side eval (no Ray dependency)\n",
    "    X_test = test_pdf.drop(columns=[label_col])\n",
    "    \n",
    "    dmat = xgb.DMatrix(X_test)\n",
    "    y_pred = booster.predict(dmat)\n",
    "    rmse = math.sqrt(((test_pdf[label_col].to_numpy() - y_pred) ** 2).mean())\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    \n",
    "    # Log final under parent\n",
    "\n",
    "    with mlflow.start_run(run_id=PARENT_RUN_ID):\n",
    "        X_example = X_test.head(5).copy()  \n",
    "        y_example = booster.predict(xgb.DMatrix(X_example))\n",
    "        sig = infer_signature(X_example, y_example)\n",
    "        with mlflow.start_run(run_name=\"final_fit\", nested=True):\n",
    "            mlflow.log_params(best.config.get(\"params\", {}))\n",
    "            mlflow.log_dict({\"label\": label_col, \"features\": feature_cols}, \"features.json\")\n",
    "            mlflow.log_metric(\"valid_rmse_best\", float(best.metrics.get(\"valid-rmse\")))\n",
    "            mlflow.log_metric(\"test_rmse\", float(rmse))\n",
    "            mlflow_xgb.log_model(booster, artifact_path=\"model\",signature=sig,input_example=X_example)\n",
    "    run = client.get_run(PARENT_RUN_ID)\n",
    "    if run.info.status == \"RUNNING\":\n",
    "        client.set_terminated(PARENT_RUN_ID, \"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daaca126-a443-402f-8b44-1c5f9a3f2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def _ensure_ray_connected(ray_envs: Dict[str,str], ray_ns:str):\n",
    "    if ray.is_initialized():\n",
    "        return\n",
    "    # Reconnect to the running cluster (prefers ray:// if present)\n",
    "    addr = None\n",
    "    if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "        addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "    ray.init(\n",
    "        address=addr or \"auto\",\n",
    "        runtime_env={\"env_vars\": ray_envs},   # same env you used earlier\n",
    "        namespace=ray_ns,\n",
    "        ignore_reinit_error=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fbea44-aaf1-4295-b041-c1351268766a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=8951, ip=100.64.3.241)\u001b[0m [12:07:07] [24]\ttrain-rmse:0.49320\tvalid-rmse:0.53827\n",
      "\u001b[36m(TunerInternal pid=88477)\u001b[0m The `callbacks.on_trial_result` operation took 1.514 s, which may be a performance bottleneck.\n",
      "\u001b[36m(TunerInternal pid=88477)\u001b[0m The `process_trial_result` operation took 1.514 s, which may be a performance bottleneck.\n",
      "\u001b[36m(TunerInternal pid=88477)\u001b[0m Processing trial results took 1.514 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[36m(TunerInternal pid=88477)\u001b[0m The `process_trial_result` operation took 1.514 s, which may be a performance bottleneck.\n",
      "\u001b[36m(TunerInternal pid=88477)\u001b[0m The `callbacks.on_trial_result` operation took 1.499 s, which may be a performance bottleneck.\n",
      "\u001b[36m(TunerInternal pid=88477)\u001b[0m The `process_trial_result` operation took 1.499 s, which may be a performance bottleneck.\n",
      "\u001b[36m(TunerInternal pid=88477)\u001b[0m Processing trial results took 1.499 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[36m(TunerInternal pid=88477)\u001b[0m The `process_trial_result` operation took 1.499 s, which may be a performance bottleneck.\n",
      "\u001b[36m(XGBoostTrainer pid=9121, ip=100.64.35.74)\u001b[0m [12:07:08] [53]\ttrain-rmse:1.09316\tvalid-rmse:1.08318\n"
     ]
    }
   ],
   "source": [
    "# ---------- cluster client init (if using Ray Client) ----------\n",
    "# If you're connecting to a running Ray cluster via ray://, keep the IRSA vars with runtime_env there too.\n",
    "ray.shutdown()\n",
    "\n",
    "RAY_JOB_ENV = {\n",
    "    \"AWS_ROLE_ARN\": os.environ.get(\"AWS_ROLE_ARN\", \"\"),\n",
    "    \"AWS_WEB_IDENTITY_TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\", \"\"),\n",
    "    \"AWS_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"AWS_DEFAULT_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\":\"1\",\n",
    "    \"TUNE_RESULT_BUFFER_LENGTH\": \"16\",\n",
    "    \"TUNE_RESULT_BUFFER_FLUSH_INTERVAL_S\": \"3\",    \n",
    "    \n",
    "}\n",
    "\n",
    "_ensure_ray_connected(RAY_JOB_ENV,ray_ns=\"xgb-eval\")\n",
    "data_dir = \"s3://ddl-wadkars/navy/california/\"\n",
    "main(data_dir=data_dir, num_workers=4, cpus_per_worker=1,DEV_FAST=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090a047-0626-4d00-9554-aba802ddf5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e00d6-0f97-4180-a994-7cb7fc0e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, sys, pyarrow as pa, pandas as pd\n",
    "print(\"DRIVER:\", sys.version)\n",
    "print(\"DRIVER pyarrow:\", pa.__version__)\n",
    "print(\"DRIVER pandas :\", pd.__version__)\n",
    "\n",
    "@ray.remote\n",
    "def _env_probe():\n",
    "    import sys, pyarrow as pa, pandas as pd\n",
    "    return {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"pyarrow\": pa.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "    }\n",
    "\n",
    "print(\"WORKER:\", ray.get(_env_probe.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24693a55-d5a2-482c-95e0-364e8d60c521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a23211-b6aa-469e-878f-b19ec0943134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd3a8b-6669-45fa-9368-dc722d74746c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
