{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a54bc1-58ed-42ae-ae0c-341a0ca2d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02ec72-8d9f-4419-9afc-bf4c261e512d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07c45f93-f246-40d0-8f49-78e0a249c4cc",
   "metadata": {},
   "source": [
    "## Create Domino Environments\n",
    "\n",
    "**Workspace Environment**\n",
    "\n",
    "**Base Env** - `quay.io/domino/domino-ray-environment:ubuntu22-py3.10-r4.4-ray2.36.0-domino6.0`\n",
    "\n",
    "**Dockerfile**\n",
    "```\n",
    "USER root\n",
    "RUN apt update && apt install -y unixodbc unixodbc-dev\n",
    "RUN pip install h11==0.16.0\n",
    "RUN pip install pyarrow==14.0.2\n",
    "RUN pip install hyperopt\n",
    "RUN pip uninstall -y bson\n",
    "RUN pip install pymongo\n",
    "RUN pip install -q \"xgboost>=2.0,<3\"\n",
    "RUN pip install hydra-core \n",
    "RUN pip install --no-cache-dir -q \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\" \"pyopenssl<24\" \"cryptography<42\"\n",
    "```\n",
    "\n",
    "Make sure to add the **pluggable workspace tools**\n",
    "```\n",
    "jupyter:\n",
    "  title: \"Jupyter (Python, R, Julia)\"\n",
    "  iconUrl: \"/assets/images/workspace-logos/Jupyter.svg\"\n",
    "  start: [ \"/opt/domino/workspaces/jupyter/start\" ]\n",
    "  supportedFileExtensions: [ \".ipynb\" ]\n",
    "  httpProxy:\n",
    "    port: 8888\n",
    "    rewrite: false\n",
    "    internalPath: \"/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}\"\n",
    "    requireSubdomain: false\n",
    "jupyterlab:\n",
    "  title: \"JupyterLab\"\n",
    "  iconUrl: \"/assets/images/workspace-logos/jupyterlab.svg\"\n",
    "  start: [  \"/opt/domino/workspaces/jupyterlab/start\" ]\n",
    "  httpProxy:\n",
    "    internalPath: \"/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}\"\n",
    "    port: 8888\n",
    "    rewrite: false\n",
    "    requireSubdomain: false\n",
    "vscode:\n",
    "  title: \"vscode\"\n",
    "  iconUrl: \"/assets/images/workspace-logos/vscode.svg\"\n",
    "  start: [ \"/opt/domino/workspaces/vscode/start\" ]\n",
    "  httpProxy:\n",
    "    port: 8888\n",
    "    requireSubdomain: false\n",
    "rstudio:\n",
    "  title: \"RStudio\"\n",
    "  iconUrl: \"/assets/images/workspace-logos/Rstudio.svg\"\n",
    "  start: [ \"/opt/domino/workspaces/rstudio/start\" ]\n",
    "  httpProxy:\n",
    "    port: 8888\n",
    "    requireSubdomain: false\n",
    "```\n",
    "  \n",
    "\n",
    "\n",
    "## Ray cluster environment\n",
    "\n",
    "**Base Env** - `quay.io/domino/ray-cluster-environment:ray2.36.0-py3.10-domino6.0`\n",
    "\n",
    "**Dockerfile**\n",
    "\n",
    "```\n",
    "USER root\n",
    "RUN apt update && apt install -y unixodbc unixodbc-dev\n",
    "RUN /opt/conda/bin/pip install h11==0.16.0\n",
    "RUN pip install pyarrow==14.0.2\n",
    "RUN pip install hyperopt\n",
    "RUN pip uninstall -y bson\n",
    "RUN pip install pymongo\n",
    "RUN pip install -q \"xgboost>=2.0,<3\"\n",
    "RUN pip install hydra-core \n",
    "RUN pip install --no-cache-dir -q \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\" \"pyopenssl<24\" \"cryptography<42\"\n",
    "USER ubuntu    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f299857-3935-4e11-9083-7e8188fc693f",
   "metadata": {},
   "source": [
    "## Start the workspace\n",
    "\n",
    "Start a workspace with a Ray cluster with 3 medium sized worker nodes and 1 small sized head node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c011f-0686-4ab2-8aca-369d7806a9b0",
   "metadata": {},
   "source": [
    "## Pick the configuration file\n",
    "\n",
    "We use hydra to run various types of run (Root folder is `/mnt/code/conf`) -\n",
    "1. `config.yaml` - Default values\n",
    "2. `env/local.yaml` - Runs in Domino via Domino datasets as a shared location. Uses a small subset of the total data\n",
    "3. `env/dev.yaml` - Runs with S3 bucket and uses a small subset of the total data\n",
    "4. `env/test.yaml` - Runs with S3 bucket and uses the full dataset\n",
    "5. `env/prod.yaml` - Runs with S3 bucket and uses the full dataset\n",
    "\n",
    "There is no difference between `test` and `prod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a093c1ef-1f20-4e1b-84de-891d32ff48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_env=\"local\" #Picks the appropriate hydra config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1336f048-16eb-441b-9c12-74309c613578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 13:34:41,673\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-10-07 13:34:41,774\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-10-07 13:34:41,919\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost as mlflow_xgb\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "from ray.data import read_parquet\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "try:\n",
    "    from ray.tune.callback import Callback      # Ray >= 2.6\n",
    "except ImportError:\n",
    "    from ray.tune.callbacks import Callback     # Older Ray\n",
    "from utils import ddl_cluster_scaling_client\n",
    "from utils import mlflow_utils\n",
    "from utils import ray_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129018e0-7750-40d8-8b4b-b0333bcc1d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fe473df-6ff9-4cce-8bbf-10f4493cb6dd",
   "metadata": {},
   "source": [
    "## Pre-requsites - When using S3 bucket\n",
    "\n",
    "Configure the following user environment variables\n",
    "\n",
    "1. AWS_ROLE_ARN - This is the AWS role being assumed via IR\n",
    "2. S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83111d82-fcdd-4b35-a491-f714007b3527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UserId': 'AROA5YW464O6S35MGC2WL:botocore-session-1759844082', 'Account': '946429944765', 'Arn': 'arn:aws:sts::946429944765:assumed-role/sameer-irsa-full-bucket-role/botocore-session-1759844082', 'ResponseMetadata': {'RequestId': 'be3a2d66-9ba1-44bf-a443-f4d80896ed58', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'be3a2d66-9ba1-44bf-a443-f4d80896ed58', 'x-amz-sts-extended-request-id': 'MTp1cy13ZXN0LTI6MTc1OTg0NDA4MjY3MjpSOkRBM3JhZ0xM', 'content-type': 'text/xml', 'content-length': '489', 'date': 'Tue, 07 Oct 2025 13:34:42 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "## Verify your role identity in AWS\n",
    "import boto3\n",
    "sts = boto3.client(\"sts\")\n",
    "identity = sts.get_caller_identity()\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46eb3e5a-835a-4942-9fbb-2bc685605ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.rename(columns={\"MedHouseVal\": \"median_house_value\"})\n",
    "\n",
    "# Split\n",
    "train, tmp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val, test  = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save locally\n",
    "train.to_parquet(\"/tmp/train.parquet\", index=False)\n",
    "val.to_parquet(\"/tmp/val.parquet\", index=False)\n",
    "test.to_parquet(\"/tmp/test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8af9bda-73fe-4dde-b2b5-c9c6e54ee9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../tmp/train.parquet to s3://ddl-wadkars/end-to-end/california/train/train.parquet\n",
      "upload: ../../../tmp/val.parquet to s3://ddl-wadkars/end-to-end/california/val/val.parquet\n",
      "upload: ../../../tmp/test.parquet to s3://ddl-wadkars/end-to-end/california/test/test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download dataset and push to S3\n",
    "# Push to S3\n",
    "!aws s3 cp /tmp/train.parquet s3://${S3_BUCKET_NAME}/end-to-end/california/train/\n",
    "!aws s3 cp /tmp/val.parquet   s3://${S3_BUCKET_NAME}/end-to-end/california/val/\n",
    "!aws s3 cp /tmp/test.parquet  s3://${S3_BUCKET_NAME}/end-to-end/california/test/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1aa7ff-e653-40af-9e36-9faa8a0ff5b3",
   "metadata": {},
   "source": [
    "## Pre-requsites - When using a Domino dataset\n",
    "\n",
    "If you are working on your own project there will be a dataset with the same name as the project. Configure the env variable\n",
    "`DATASET_NAME` below to that dataset name. If not associate a dataset you have permissions to for your project and name the \n",
    "environment variable accordingly.\n",
    "\n",
    "Also note that the `ROOT_DOMINO_DATASET_FOLDER` is `/mnt/data` for a git backed project. For a Domino File System backed project it is `/domino/datasets/local/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030ee63e-3378-4777-ad4b-0a0192a448d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DOMINO_DATASET_FOLDER = \"/mnt/data\" #For DFS based project use /domino/datasets/local/\n",
    "DATASET_NAME=\"ddl-end-to-end-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1897eb5-a71b-41b1-b8d9-2e85a0709eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /tmp/train.parquet -> /mnt/data/ddl-end-to-end-demo/end-to-end/california/train\n",
      "Copied /tmp/val.parquet -> /mnt/data/ddl-end-to-end-demo/end-to-end/california/val\n",
      "Copied /tmp/test.parquet -> /mnt/data/ddl-end-to-end-demo/end-to-end/california/test\n"
     ]
    }
   ],
   "source": [
    "def copy_file_with_dirs(src_file, dest_file):\n",
    "    \"\"\"\n",
    "    Copy a file from src_file to dest_file, creating destination\n",
    "    folders if they don't exist.\n",
    "    \"\"\"\n",
    "    # Ensure destination directory exists\n",
    "    dest_dir = os.path.dirname(dest_file)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    # Copy the file\n",
    "    shutil.copy2(src_file, dest_file)  # copy2 preserves metadata\n",
    "    print(f\"Copied {src_file} -> {dest_file}\")\n",
    "\n",
    "\n",
    "src = f\"/tmp/train.parquet\"\n",
    "dest = f\"{ROOT_DOMINO_DATASET_FOLDER}/{DATASET_NAME}/end-to-end/california/train\"\n",
    "copy_file_with_dirs(src, dest)\n",
    "\n",
    "src = f\"/tmp/val.parquet\"\n",
    "dest = f\"{ROOT_DOMINO_DATASET_FOLDER}/{DATASET_NAME}/end-to-end/california/val\"\n",
    "copy_file_with_dirs(src, dest)\n",
    "\n",
    "src = f\"/tmp/test.parquet\"\n",
    "dest = f\"{ROOT_DOMINO_DATASET_FOLDER}/{DATASET_NAME}/end-to-end/california/test\"\n",
    "copy_file_with_dirs(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3677c58-b14b-4e5e-9248-7db95e5ff8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numbers\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from ray.tune.experiment.trial import Trial\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback as _BaseMLflowLoggerCallback\n",
    "\n",
    "\n",
    "def _flatten(d: Dict[str, Any], parent: str = \"\", sep: str = \".\") -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        nk = f\"{parent}{sep}{k}\" if parent else str(k)\n",
    "        if isinstance(v, dict):\n",
    "            out.update(_flatten(v, nk, sep))\n",
    "        elif isinstance(v, (list, tuple)):\n",
    "            s = str(v[:64]) + (\"...\" if len(v) > 64 else \"\")\n",
    "            out[nk] = s\n",
    "        else:\n",
    "            out[nk] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "def _is_num(x: Any) -> bool:\n",
    "    return isinstance(x, numbers.Number) and not (isinstance(x, float) and (math.isnan(x) or math.isinf(x)))\n",
    "\n",
    "\n",
    "class ChildRunMLflowCallback(_BaseMLflowLoggerCallback):\n",
    "    \"\"\"\n",
    "    A strict child-run MLflow logger for Ray Tune that preserves the public API of\n",
    "    ray.air.integrations.mlflow.MLflowLoggerCallback, but:\n",
    "      - Nests each trial under an optional parent_run_id.\n",
    "      - Logs all numeric metrics per step.\n",
    "      - Logs flattened trial config as params once at trial start.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tracking_uri: Optional[str] = None,\n",
    "        experiment_name: Optional[str] = None,\n",
    "        save_artifact: bool = True,\n",
    "        tags: Optional[Dict[str, str]] = None,\n",
    "        *,\n",
    "        parent_run_id: Optional[str] = None,\n",
    "        run_name_prefix: str = \"trial\",\n",
    "    ):\n",
    "        self.parent_run_id = parent_run_id\n",
    "        self.run_name_prefix = run_name_prefix\n",
    "\n",
    "        merged_tags = dict(tags or {})\n",
    "        if parent_run_id and \"mlflow.parentRunId\" not in merged_tags:\n",
    "            merged_tags[\"mlflow.parentRunId\"] = parent_run_id\n",
    "\n",
    "        # init base (keeps compatibility with Ray’s signature/behavior)\n",
    "        super().__init__(\n",
    "            tracking_uri=tracking_uri,\n",
    "            experiment_name=experiment_name,\n",
    "            save_artifact=save_artifact,\n",
    "            tags=merged_tags,\n",
    "        )\n",
    "\n",
    "        # Our own bookkeeping\n",
    "        self._client: Optional[MlflowClient] = None\n",
    "        self._exp_id: Optional[str] = None\n",
    "        self._trial_run_ids: Dict[str, str] = {}\n",
    "        self._trial_steps: Dict[str, int] = {}\n",
    "\n",
    "        # Make workers inherit if needed\n",
    "        if tracking_uri:\n",
    "            os.environ[\"MLFLOW_TRACKING_URI\"] = tracking_uri\n",
    "\n",
    "    # ---- Ray callback hooks ----\n",
    "\n",
    "    def setup(self, **info):\n",
    "        # Mirror Ray’s built-in setup, but we keep explicit handles around.\n",
    "        if self.tracking_uri:\n",
    "            mlflow.set_tracking_uri(self.tracking_uri)\n",
    "        if self.experiment_name:\n",
    "            mlflow.set_experiment(self.experiment_name)\n",
    "\n",
    "        self._client = MlflowClient(self.tracking_uri or mlflow.get_tracking_uri())\n",
    "        if self.experiment_name:\n",
    "            exp = self._client.get_experiment_by_name(self.experiment_name)\n",
    "            self._exp_id = exp.experiment_id if exp else self._client.create_experiment(self.experiment_name)\n",
    "        else:\n",
    "            # fall back to the current active experiment\n",
    "            self._exp_id = mlflow.get_experiment_by_name(mlflow.get_experiment(mlflow.active_run().info.experiment_id).name).experiment_id  # type: ignore\n",
    "\n",
    "    def on_trial_start(self, iteration: int, trials: List[Trial], trial: Trial, **info):\n",
    "        assert self._client and self._exp_id, \"MLflow not initialized. Did setup() run?\"\n",
    "        run_name = f\"{self.run_name_prefix}-{trial.trial_id[:8]}\"\n",
    "        tags = dict(self.tags or {})\n",
    "        tags.setdefault(\"mlflow.runName\", run_name)\n",
    "        tags[\"source\"] = \"ray.tune\"\n",
    "        tags[\"ray.trial_id\"] = trial.trial_id\n",
    "\n",
    "        run = self._client.create_run(experiment_id=self._exp_id, tags=tags)\n",
    "        run_id = run.info.run_id\n",
    "        self._trial_run_ids[trial.trial_id] = run_id\n",
    "        self._trial_steps[trial.trial_id] = 0\n",
    "\n",
    "        # Params once\n",
    "        for k, v in _flatten(trial.config).items():\n",
    "            try:\n",
    "                self._client.log_param(run_id, k, str(v))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def on_trial_result(self, iteration: int, trials: List[Trial], trial: Trial, result: Dict[str, Any], **info):\n",
    "        run_id = self._trial_run_ids.get(trial.trial_id)\n",
    "        if not run_id:\n",
    "            return\n",
    "\n",
    "        step = int(result.get(\"training_iteration\", self._trial_steps[trial.trial_id]))\n",
    "        self._trial_steps[trial.trial_id] = step + 1\n",
    "\n",
    "        metrics = {k: float(v) for k, v in result.items() if _is_num(v)}\n",
    "        if not metrics:\n",
    "            return\n",
    "\n",
    "        # Batch log for efficiency\n",
    "        self._client.log_batch(\n",
    "            run_id=run_id,\n",
    "            metrics=[mlflow.entities.Metric(key=k, value=v, timestamp=None, step=step) for k, v in metrics.items()],\n",
    "        )\n",
    "\n",
    "    def on_trial_complete(self, iteration: int, trials: List[Trial], trial: Trial, **info):\n",
    "        self._finish(trial, status=\"FINISHED\")\n",
    "\n",
    "    def on_trial_error(self, iteration: int, trials: List[Trial], trial: Trial, **info):\n",
    "        self._finish(trial, status=\"FAILED\")\n",
    "\n",
    "    def on_experiment_end(self, trials: List[Trial], **info):\n",
    "        for t in list(self._trial_run_ids.keys()):\n",
    "            # close any dangling runs\n",
    "            fake = type(\"T\", (), {\"trial_id\": t})\n",
    "            self._finish(fake, status=\"FINISHED\")\n",
    "\n",
    "    # ---- internals ----\n",
    "\n",
    "    def _finish(self, trial: Trial, status: str):\n",
    "        run_id = self._trial_run_ids.pop(trial.trial_id, None)\n",
    "        if not run_id or not self._client:\n",
    "            return\n",
    "        try:\n",
    "            self._client.set_terminated(run_id, status=status)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5721e5e3-191f-470a-abfc-5cdcd87523a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air._internal.mlflow import _MLflowLoggerUtil\n",
    "from ray.tune.logger import LoggerCallback\n",
    "from ray.tune.result import TIMESTEPS_TOTAL\n",
    "from ray.tune.experiment import Trial\n",
    "from ray.util.annotations import PublicAPI\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "class MyMLflowLoggerCallback(LoggerCallback):\n",
    "    \"\"\"MLflow Logger to automatically log Tune results and config to MLflow.\n",
    "\n",
    "    MLflow (https://mlflow.org) Tracking is an open source library for\n",
    "    recording and querying experiments. This Ray Tune ``LoggerCallback``\n",
    "    sends information (config parameters, training results & metrics,\n",
    "    and artifacts) to MLflow for automatic experiment tracking.\n",
    "\n",
    "    Args:\n",
    "        tracking_uri: The tracking URI for where to manage experiments\n",
    "            and runs. This can either be a local file path or a remote server.\n",
    "            This arg gets passed directly to mlflow\n",
    "            initialization. When using Tune in a multi-node setting, make sure\n",
    "            to set this to a remote server and not a local file path.\n",
    "        registry_uri: The registry URI that gets passed directly to\n",
    "            mlflow initialization.\n",
    "        experiment_name: The experiment name to use for this Tune run.\n",
    "            If the experiment with the name already exists with MLflow,\n",
    "            it will be reused. If not, a new experiment will be created with\n",
    "            that name.\n",
    "        tags: An optional dictionary of string keys and values to set\n",
    "            as tags on the run\n",
    "        tracking_token: Tracking token used to authenticate with MLflow.\n",
    "        save_artifact: If set to True, automatically save the entire\n",
    "            contents of the Tune local_dir as an artifact to the\n",
    "            corresponding run in MlFlow.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "\n",
    "        tags = { \"user_name\" : \"John\",\n",
    "                 \"git_commit_hash\" : \"abc123\"}\n",
    "\n",
    "        tune.run(\n",
    "            train_fn,\n",
    "            config={\n",
    "                # define search space here\n",
    "                \"parameter_1\": tune.choice([1, 2, 3]),\n",
    "                \"parameter_2\": tune.choice([4, 5, 6]),\n",
    "            },\n",
    "            callbacks=[MLflowLoggerCallback(\n",
    "                experiment_name=\"experiment1\",\n",
    "                tags=tags,\n",
    "                save_artifact=True)])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tracking_uri: Optional[str] = None,\n",
    "        *,\n",
    "        registry_uri: Optional[str] = None,\n",
    "        experiment_name: Optional[str] = None,\n",
    "        tags: Optional[Dict] = None,\n",
    "        tracking_token: Optional[str] = None,\n",
    "        save_artifact: bool = False,\n",
    "    ):\n",
    "\n",
    "        self.tracking_uri = tracking_uri\n",
    "        self.registry_uri = registry_uri\n",
    "        self.experiment_name = experiment_name\n",
    "        self.tags = tags\n",
    "        self.tracking_token = tracking_token\n",
    "        self.should_save_artifact = save_artifact\n",
    "\n",
    "        self.mlflow_util = _MLflowLoggerUtil()\n",
    "        self.parent_run_id = ''\n",
    "        if ray.util.client.ray.is_connected():\n",
    "            logger.warning(\n",
    "                \"When using MLflowLoggerCallback with Ray Client, \"\n",
    "                \"it is recommended to use a remote tracking \"\n",
    "                \"server. If you are using a MLflow tracking server \"\n",
    "                \"backed by the local filesystem, then it must be \"\n",
    "                \"setup on the server side and not on the client \"\n",
    "                \"side.\"\n",
    "            )\n",
    "    def log_trial_result(self, iteration: int, trial: \"Trial\", result: Dict):\n",
    "        #step = result.get(TIMESTEPS_TOTAL) or result[TRAINING_ITERATION]\n",
    "        #run_id = self._trial_runs[trial]\n",
    "        #print(mlflow.active_run())\n",
    "        #run_id = mlflow.active_run().info.run_id\n",
    "        self.mlflow_util.log_metrics(metrics_to_log=result, step=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1adda697-3841-450e-98f4-bed31db79159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ray.train.callbacks import JsonLoggerCallback\n",
    "\n",
    "def _s3p(root: str, sub: str) -> str:\n",
    "    \"\"\"Safe join for S3/posix URIs.\"\"\"\n",
    "    val = f\"{root.rstrip('/')}/{sub.lstrip('/')}\"\n",
    "    return val \n",
    "\n",
    "\n",
    "def read_parquet_to_pandas(uri: str, columns=None, limit: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust Parquet→pandas loader that bypasses Ray Data.\n",
    "    Works with local paths and s3:// (PyArrow uses AWS_* env vars / IRSA).\n",
    "    \"\"\"\n",
    "    ds = pds.dataset(uri.rstrip(\"/\"), format=\"parquet\")\n",
    "    if limit is None:\n",
    "        return ds.to_table(columns=columns).to_pandas()\n",
    "\n",
    "    # Respect limit across files/row groups\n",
    "    scanner = pds.Scanner.from_dataset(ds, columns=columns)\n",
    "    batches, rows = [], 0\n",
    "    for b in scanner.to_batches():\n",
    "        batches.append(b)\n",
    "        rows += len(b)\n",
    "        if rows >= limit:\n",
    "            return pa.Table.from_batches(batches)[:limit].to_pandas()\n",
    "    return pa.Table.from_batches(batches).to_pandas()\n",
    "\n",
    "\n",
    "def main(experiment_name:str,data_dir: str,\n",
    "         model_name:str,model_desc:str,\n",
    "         num_workers: int = 4, cpus_per_worker: int = 1,  DEV_FAST: bool = False):\n",
    "    \"\"\"\n",
    "    Quick knobs:\n",
    "      - num_workers * cpus_per_worker = CPUs per trial.\n",
    "      - trainer_resources={\"CPU\":0} so the driver doesn't steal a core.\n",
    "      - PACK placement to keep trials tight.\n",
    "      - max_concurrent_trials caps parallel trials.\n",
    "      - num_boost_round / early_stopping_rounds control trial length.\n",
    "      - nthread = cpus_per_worker to avoid oversubscription.\n",
    "    \"\"\"\n",
    "    mlflow.xgboost.autolog() \n",
    "    exp_id = mlflow_utils.ensure_mlflow_experiment(experiment_name)\n",
    "    mv = mlflow_utils.ensure_registered_model(model_name)\n",
    "    # Storage: local for dev, S3/your env otherwise\n",
    "    RUN_STORAGE = os.environ.get(\"RAY_AIR_STORAGE\", f\"{data_dir}/air/xgb\")\n",
    "    TUNER_STORAGE = \"/tmp/air-dev\" if DEV_FAST else RUN_STORAGE\n",
    "    FINAL_STORAGE = \"/mnt/data/ddl-end-to-end-demo/air/final_fit\" if DEV_FAST else RUN_STORAGE\n",
    "\n",
    "    # Sanity: workers see IRSA env?\n",
    "    @ray.remote\n",
    "    def _peek():\n",
    "        import os\n",
    "        return {\n",
    "            \"ROLE\": bool(os.environ.get(\"AWS_ROLE_ARN\")),\n",
    "            \"TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\"),\n",
    "            \"REGION\": os.environ.get(\"AWS_REGION\"),\n",
    "        }\n",
    "    print(\"Worker env peek:\", ray.get(_peek.remote()))\n",
    "\n",
    "    # MLflow (experiment + parent run)\n",
    "    CLUSTER_TRACKING_URI = os.environ[\"CLUSTER_MLFLOW_TRACKING_URI\"]\n",
    "    \n",
    "    \n",
    "    client = MlflowClient()\n",
    "\n",
    "\n",
    "    parent = client.create_run(\n",
    "        experiment_id=exp_id,\n",
    "        tags={\"mlflow.runName\": \"xgb_parent\", \"role\": \"tune_parent\"},\n",
    "    )\n",
    "    parent_run_id = parent.info.run_id\n",
    "    print(\"Parent run id:\", parent_run_id)\n",
    "\n",
    "    # Data (Ray Datasets for training/val)\n",
    "    train_ds = read_parquet(_s3p(data_dir, \"train\"), parallelism=num_workers)\n",
    "    val_ds   = read_parquet(_s3p(data_dir, \"val\"),   parallelism=num_workers)\n",
    "    test_ds  = read_parquet(_s3p(data_dir, \"test\"),  parallelism=num_workers)\n",
    "    print(\"Schema:\", train_ds.schema())\n",
    "\n",
    "    # Label + features\n",
    "    label_col = \"median_house_value\"\n",
    "    feature_cols = [c for c in train_ds.schema().names if c != label_col]\n",
    "    keep = feature_cols + [label_col]\n",
    "    train_ds = train_ds.select_columns(keep)\n",
    "    val_ds   = val_ds.select_columns(keep)\n",
    "\n",
    "    # DEV: trim Ray Datasets used for training; eval will bypass Ray entirely\n",
    "    if DEV_FAST:\n",
    "        train_ds = train_ds.limit(5_000)\n",
    "        val_ds   = val_ds.limit(2_000)\n",
    "\n",
    "    # --- Build test DataFrame without Ray (avoids 'Global node is not initialized') ---\n",
    "    test_uri = _s3p(data_dir, \"test\")\n",
    "    test_pdf = read_parquet_to_pandas(\n",
    "        test_uri, columns=keep, limit=2_000 if DEV_FAST else None\n",
    "    )\n",
    "\n",
    "    # Search space\n",
    "    param_space = {\n",
    "        \"params\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": tune.loguniform(1e-3, 3e-1),\n",
    "            \"max_depth\": tune.randint(4, 12),\n",
    "            \"min_child_weight\": tune.loguniform(1e-2, 10),\n",
    "            \"subsample\": tune.uniform(0.6, 1.0),\n",
    "            \"colsample_bytree\": tune.uniform(0.6, 1.0),\n",
    "            \"lambda\": tune.loguniform(1e-3, 10),\n",
    "            \"alpha\": tune.loguniform(1e-3, 10),\n",
    "        },\n",
    "        \"num_boost_round\": 300,\n",
    "        \"early_stopping_rounds\": 20,\n",
    "    }\n",
    "\n",
    "    # Dev shortcuts\n",
    "    if DEV_FAST:\n",
    "        param_space[\"num_boost_round\"] = 20\n",
    "        param_space[\"early_stopping_rounds\"] = 5\n",
    "        #num_workers = 1\n",
    "        #cpus_per_worker = 1\n",
    "        NUM_SAMPLES = 5\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "    else:\n",
    "        NUM_SAMPLES = 30\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "\n",
    "    # Threads per worker\n",
    "    param_space[\"params\"][\"nthread\"] = cpus_per_worker\n",
    "    print(\"Per-trial CPUs =\", num_workers * cpus_per_worker)\n",
    "\n",
    "    # Scaling / placement\n",
    "    scaling = ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=False,\n",
    "        resources_per_worker={\"CPU\": cpus_per_worker},\n",
    "        trainer_resources={\"CPU\": 0},\n",
    "        placement_strategy=\"PACK\",\n",
    "    )\n",
    "\n",
    "    # Trainable\n",
    "    trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=param_space[\"params\"],\n",
    "        datasets={\"train\": train_ds, \"valid\": val_ds},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "    )\n",
    "\n",
    "    # Search + scheduler\n",
    "    MAX_T = int(param_space[\"num_boost_round\"])\n",
    "    GRACE = int(min(param_space.get(\"early_stopping_rounds\", 1), MAX_T))\n",
    "    algo = HyperOptSearch(metric=\"valid-rmse\", mode=\"min\")\n",
    "    scheduler = ASHAScheduler(max_t=MAX_T, grace_period=GRACE, reduction_factor=3)\n",
    "\n",
    "    # MLflow callback (child runs)\n",
    "    \n",
    "    mlflow_cb = MLflowLoggerCallback(\n",
    "        tracking_uri=CLUSTER_TRACKING_URI,\n",
    "        experiment_name=experiment_name,\n",
    "        save_artifact=SAVE_ARTIFACTS,\n",
    "        #log_params_on_trial_end=True,\n",
    "        tags={\"mlflow.parentRunId\": parent_run_id},\n",
    "    )\n",
    "    '''\n",
    "    mlflow_cb = ChildRunMLflowCallback(\n",
    "        tracking_uri=CLUSTER_TRACKING_URI,\n",
    "        experiment_name=experiment_name,\n",
    "        save_artifact=True,                         # same flag name as Ray's\n",
    "        #tags={\"project\": \"xgb_from_s3_irsa\"},       # optional; parent added automatically\n",
    "        tags={\"mlflow.parentRunId\": parent_run_id},\n",
    "        parent_run_id=parent_run_id,\n",
    "        run_name_prefix=\"xgb-trial\",\n",
    "        \n",
    "    )\n",
    "    '''\n",
    "    # Tuner\n",
    "    tuner = tune.Tuner(\n",
    "        trainer.as_trainable(),\n",
    "        run_config=RunConfig(\n",
    "            name=\"xgb_from_s3_irsa\",\n",
    "            storage_path=TUNER_STORAGE,\n",
    "            callbacks=[mlflow_cb],\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            metric=\"valid-rmse\",\n",
    "            mode=\"min\",\n",
    "            num_samples=NUM_SAMPLES,\n",
    "            max_concurrent_trials=MAX_CONCURRENT,\n",
    "        ),\n",
    "        param_space={\"params\": param_space[\"params\"]},\n",
    "    )\n",
    "\n",
    "    # Tune\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"valid-rmse\", mode=\"min\")\n",
    "    print(\"Best config:\", best.config)\n",
    "    print(\"Best valid RMSE:\", best.metrics.get(\"valid-rmse\"))\n",
    "\n",
    "    # Final fit (train + val)\n",
    "    merged = train_ds.union(val_ds)\n",
    "    final_trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=best.config[\"params\"],\n",
    "        datasets={\"train\": merged},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "        run_config=RunConfig(name=\"final_fit\", storage_path=FINAL_STORAGE),\n",
    "    )\n",
    "    final_result = final_trainer.fit()\n",
    "    final_ckpt = final_result.checkpoint\n",
    "\n",
    "    # Load Booster from checkpoint\n",
    "    with final_ckpt.as_directory() as ckpt_dir:\n",
    "        print(\"Checkpoint dir:\", ckpt_dir, \"files:\", os.listdir(ckpt_dir))\n",
    "        candidates = [\"model.json\", \"model.ubj\", \"model.xgb\", \"xgboost_model.json\", \"model\"]\n",
    "        model_path = next(\n",
    "            (os.path.join(ckpt_dir, f) for f in candidates if os.path.exists(os.path.join(ckpt_dir, f))),\n",
    "            None,\n",
    "        )\n",
    "        if not model_path:\n",
    "            raise FileNotFoundError(f\"No XGBoost model file found in checkpoint dir: {ckpt_dir}\")\n",
    "        booster = xgb.Booster()\n",
    "        print(f\"MODEL PATH {model_path}\")\n",
    "        booster.load_model(model_path)\n",
    "\n",
    "    # Driver-side eval (no Ray dependency)\n",
    "    X_test = test_pdf.drop(columns=[label_col])\n",
    "    \n",
    "    dmat = xgb.DMatrix(X_test)\n",
    "    y_pred = booster.predict(dmat)\n",
    "    rmse = math.sqrt(((test_pdf[label_col].to_numpy() - y_pred) ** 2).mean())\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    \n",
    "    # Log final under parent\n",
    "\n",
    "    with mlflow.start_run(run_id=parent_run_id):\n",
    "        X_example = X_test.head(5).copy()  \n",
    "        y_example = booster.predict(xgb.DMatrix(X_example))\n",
    "        sig = infer_signature(X_example, y_example)\n",
    "        with mlflow.start_run(run_name=\"final_fit\", nested=True) as final_run:\n",
    "            mlflow.log_params(best.config.get(\"params\", {}))\n",
    "            mlflow.log_dict({\"label\": label_col, \"features\": feature_cols}, \"features.json\")\n",
    "            mlflow.log_metric(\"valid_rmse_best\", float(best.metrics.get(\"valid-rmse\")))\n",
    "            mlflow.log_metric(\"test_rmse\", float(rmse))\n",
    "            model_info = mlflow_xgb.log_model(booster, artifact_path=\"model\",signature=sig,input_example=X_example)\n",
    "\n",
    "            mv = mlflow_utils.register_model_version(model_name=model_name,model_desc=model_desc,\n",
    "                                                model_info=model_info,run=final_run)\n",
    "            \n",
    "            print(\"Name: {}\".format(mv.name))\n",
    "            print(\"Version: {}\".format(mv.version))\n",
    "            print(\"Description: {}\".format(mv.description))\n",
    "            print(\"Status: {}\".format(mv.status))\n",
    "            print(\"Stage: {}\".format(mv.current_stage))\n",
    "            \n",
    "    \n",
    "    run = client.get_run(parent_run_id)\n",
    "    if run.info.status == \"RUNNING\":\n",
    "        client.set_terminated(parent_run_id, \"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149bc2b-690e-4cc7-8408-225f28f2b55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2522d26e-854b-43f2-b874-ecd7be87d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_env=\"dev\" #Picks the appropriate hydra config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac0c61b-69d8-4686-858b-67ad14a2d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_773/1217648743.py:6: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../conf\"):\n"
     ]
    }
   ],
   "source": [
    "## Read Conf from Hydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "# Point Hydra to your conf/ directory\n",
    "with initialize(config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config\", overrides=[f\"env={which_env}\"])\n",
    "    #print(f\"Running in {cfg.env} environment\")\n",
    "    #print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "    \n",
    "    app_name = cfg.app.name\n",
    "\n",
    "    #For dev we use datasets\n",
    "    data_dir = cfg.app.data_dir\n",
    "    experiment_name = cfg.mlflow.experiment_name    \n",
    "    model_name = cfg.mlflow.model_name    \n",
    "    model_desc = cfg.mlflow.model_desc\n",
    "    ray_workers = cfg.env.ray.num_workers\n",
    "    cpus_per_worker = cfg.env.ray.cpus_per_worker\n",
    "    dev_fast = cfg.env.ray.dev_fast\n",
    "    #print(ray_workers)\n",
    "    #print(dev_fast)\n",
    "    \n",
    "# Disable tensorboard integration\n",
    "os.environ[\"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "224ba8ff-d2f0-4ad2-83b8-501b4dc41640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m View detailed results here: /mnt/data/ddl-end-to-end-demo/air/final_fit/final_fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training started without custom configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m - (node_id=8d750bcc9055c1f84d9d50c044091e65cbb25753ee8284c2ec9635fe, ip=100.64.74.230, pid=751) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m - (node_id=0e1126f0ada99ca3098b1a24f119d3e0867af419d9ec0873ac17743b, ip=100.64.80.25, pid=793) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m - (node_id=b574e06f0b8b558aa7d02fb0c509388d27d20f522c7b7e403af587f3, ip=100.64.84.191, pid=502) world_rank=2, local_rank=0, node_rank=2\n",
      "\u001b[36m(RayTrainWorker pid=502, ip=100.64.84.191)\u001b[0m [06:40:19] Task [xgboost.ray-rank=00000002]:14c8513b08e1ba32a84ef40f03000000 got rank 2\n",
      "\u001b[36m(RayTrainWorker pid=751, ip=100.64.74.230)\u001b[0m [06:40:19] Task [xgboost.ray-rank=00000000]:8d74fc7fd1f0b5098a98715303000000 got rank 0\n",
      "\u001b[36m(RayTrainWorker pid=793, ip=100.64.80.25)\u001b[0m [06:40:19] Task [xgboost.ray-rank=00000001]:602869b0ba7dc44daf039d2003000000 got rank 1\n",
      "\u001b[36m(SplitCoordinator pid=794, ip=100.64.74.230)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-10-07_06-32-48_982510_1/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=794, ip=100.64.74.230)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(select_columns)] -> LimitOperator[limit=5000], InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(select_columns)] -> LimitOperator[limit=2000] -> UnionOperator[UnionOperator(limit=5000, limit=2000)] -> OutputSplitter[split(3, equal=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 1 at 2025-10-07 06:40:23. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      6.81435 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          6.81435 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          1 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            1.09091 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 2 at 2025-10-07 06:40:23. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03069 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          6.84504 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          2 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            1.03083 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:23] [0]\ttrain-rmse:1.09091\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:23] [1]\ttrain-rmse:1.03083\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [2]\ttrain-rmse:0.97488\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [3]\ttrain-rmse:0.92443\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [4]\ttrain-rmse:0.87589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 3 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.02954 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          6.87458 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          3 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.97488 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 4 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.04184 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          6.91642 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          4 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.92443 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 5 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.02887 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          6.94529 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          5 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.87589 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 6 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.02837 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          6.97366 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          6 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.83132 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 7 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03172 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.00538 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          7 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.79232 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 8 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03146 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.03685 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          8 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.75313 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [5]\ttrain-rmse:0.83132\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [6]\ttrain-rmse:0.79232\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [7]\ttrain-rmse:0.75313\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [8]\ttrain-rmse:0.71597\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [9]\ttrain-rmse:0.68238\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [10]\ttrain-rmse:0.65267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 9 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03359 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.07043 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration          9 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.71597 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 10 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.02751 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.09794 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         10 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.68238 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 11 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03192 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.12987 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         11 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.65267 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 12 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03634 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.16621 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         12 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse             0.6223 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 13 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.02931 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.19552 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         13 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.59716 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 14 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.02886 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.22438 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         14 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.57419 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [11]\ttrain-rmse:0.62230\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [12]\ttrain-rmse:0.59716\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [13]\ttrain-rmse:0.57419\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [14]\ttrain-rmse:0.54923\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [15]\ttrain-rmse:0.52705\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [16]\ttrain-rmse:0.50635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 15 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.02914 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.25352 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         15 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.54923 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 16 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03135 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.28486 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         16 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.52705 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 17 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03162 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.31648 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         17 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.50635 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 18 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03063 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.34711 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         18 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse             0.4866 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 19 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.02651 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.37362 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         19 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.47074 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 20 at 2025-10-07 06:40:24. Total running time: 9s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s      0.03149 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s          7.40511 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration         20 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse            0.45359 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰───────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [17]\ttrain-rmse:0.48660\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [18]\ttrain-rmse:0.47074\n",
      "\u001b[36m(XGBoostTrainer pid=707, ip=100.64.74.230)\u001b[0m [06:40:24] [19]\ttrain-rmse:0.45359\n",
      "\u001b[36m(RayTrainWorker pid=751, ip=100.64.74.230)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/data/ddl-end-to-end-demo/air/final_fit/final_fit/XGBoostTrainer_27700_00000_0_2025-10-07_06-40-14/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training finished iteration 21 at 2025-10-07 06:40:24. Total running time: 10s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╭─────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ Training result                         │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ├─────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ checkpoint_dir_name   checkpoint_000000 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_this_iter_s                0.07184 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ time_total_s                    7.47695 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ training_iteration                   21 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m │ train-rmse                      0.45359 │\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m ╰─────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training saved a checkpoint for iteration 21 at: (local)/mnt/data/ddl-end-to-end-demo/air/final_fit/final_fit/XGBoostTrainer_27700_00000_0_2025-10-07_06-40-14/checkpoint_000000\n",
      "Checkpoint dir: /mnt/data/ddl-end-to-end-demo/air/final_fit/final_fit/XGBoostTrainer_27700_00000_0_2025-10-07_06-40-14/checkpoint_000000 files: ['model.ubj']\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Training completed after 21 iterations at 2025-10-07 06:40:25. Total running time: 11s\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m \n",
      "Test RMSE: 0.5886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Wrote the latest version of all result files and experiment state to '/mnt/data/ddl-end-to-end-demo/air/final_fit/final_fit' in 0.0263s.\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m Failed to fetch metrics for 1 trial(s):\n",
      "\u001b[36m(TunerInternal pid=2920)\u001b[0m - XGBoostTrainer_27700_00000: FileNotFoundError('Could not fetch metrics for XGBoostTrainer_27700_00000: both result.json and progress.csv were not found at /mnt/data/ddl-end-to-end-demo/air/final_fit/final_fit/XGBoostTrainer_27700_00000_0_2025-10-07_06-40-14')\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [13:40:27] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2025/10/07 13:40:30 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ray-xgboost-dev-wadkars, version 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ray-xgboost-dev-wadkars\n",
      "Version: 8\n",
      "Description: ray-xgboost-dev-wadkars\n",
      "Status: READY\n",
      "Stage: None\n",
      "🏃 View run final_fit at: http://127.0.0.1:8765/#/experiments/264/runs/87f9152b41b849a4bba4d954bd857ae7\n",
      "🧪 View experiment at: http://127.0.0.1:8765/#/experiments/264\n",
      "🏃 View run xgb_parent at: http://127.0.0.1:8765/#/experiments/264/runs/44235d7186724f37bdce88fa1391d60d\n",
      "🧪 View experiment at: http://127.0.0.1:8765/#/experiments/264\n"
     ]
    }
   ],
   "source": [
    "## All AWS Env variables are redundant when using locally\n",
    "\n",
    "RAY_JOB_ENV = {\n",
    "    \"AWS_ROLE_ARN\": os.environ.get(\"AWS_ROLE_ARN\", \"\"),\n",
    "    \"AWS_WEB_IDENTITY_TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\", \"\"),\n",
    "    \"AWS_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"AWS_DEFAULT_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\":\"1\",\n",
    "    \"TUNE_RESULT_BUFFER_LENGTH\": \"16\",\n",
    "    \"TUNE_RESULT_BUFFER_FLUSH_INTERVAL_S\": \"3\",    \n",
    "    \n",
    "}\n",
    "ray.shutdown()\n",
    "ray_utils.ensure_ray_connected(RAY_JOB_ENV,ray_ns=app_name)\n",
    "mlflow.xgboost.autolog() \n",
    "main(experiment_name=experiment_name,data_dir=data_dir, \n",
    "     model_name=model_name,model_desc=model_desc,\n",
    "     num_workers=ray_workers, cpus_per_worker=cpus_per_worker,DEV_FAST=dev_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7b117-675f-450b-8076-fcef8bf7771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mlflow_utils\n",
    "import pandas as pd\n",
    "import mlflow.pyfunc\n",
    "\n",
    "my_model = mlflow_utils.load_registered_model_version(model_name,\"latest\")\n",
    "\n",
    "# your split-style payload\n",
    "split = {\n",
    "  \"columns\": [\"MedInc\",\"HouseAge\",\"AveRooms\",\"AveBedrms\",\"Population\",\"AveOccup\",\"Latitude\",\"Longitude\"],\n",
    "  \"data\": [\n",
    "    [3.1333,30.0,5.925531914893617,1.1312056737588652,966.0,3.425531914893617,36.51,-119.65],\n",
    "    [2.3355,18.0,5.711722488038277,1.0598086124401913,1868.0,2.2344497607655502,33.97,-117.01],\n",
    "    [3.3669,29.0,4.5898778359511345,1.0767888307155322,1071.0,1.869109947643979,34.15,-118.37],\n",
    "    [3.875,46.0,4.0,1.0,59.0,4.538461538461538,33.12,-117.11],\n",
    "    [4.3482,9.0,5.7924528301886795,1.1037735849056605,409.0,1.929245283018868,35.36,-119.06]\n",
    "  ]\n",
    "}\n",
    "\n",
    "# make a DataFrame\n",
    "X = pd.DataFrame(split[\"data\"], columns=split[\"columns\"])\n",
    "preds = my_model.predict(X)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e00d6-0f97-4180-a994-7cb7fc0e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, sys, pyarrow as pa, pandas as pd\n",
    "print(\"DRIVER:\", sys.version)\n",
    "print(\"DRIVER pyarrow:\", pa.__version__)\n",
    "print(\"DRIVER pandas :\", pd.__version__)\n",
    "\n",
    "@ray.remote\n",
    "def _env_probe():\n",
    "    import sys, pyarrow as pa, pandas as pd\n",
    "    return {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"pyarrow\": pa.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "    }\n",
    "\n",
    "print(\"WORKER:\", ray.get(_env_probe.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd9c4b-94c5-44b8-9a58-51cbfbc16361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5626f7-85b6-4ae7-89bb-67467c463464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
