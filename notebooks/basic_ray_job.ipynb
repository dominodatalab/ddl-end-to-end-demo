{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a54bc1-58ed-42ae-ae0c-341a0ca2d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c45f93-f246-40d0-8f49-78e0a249c4cc",
   "metadata": {},
   "source": [
    "## Create Domino Environments\n",
    "\n",
    "**Workspace Environment**\n",
    "\n",
    "**Base Env** - `quay.io/domino/domino-ray-environment:ubuntu22-py3.10-r4.4-ray2.36.0-domino6.0`\n",
    "\n",
    "**Dockerfile**\n",
    "```\n",
    "USER root\n",
    "RUN apt update && apt install -y unixodbc unixodbc-dev\n",
    "RUN pip install h11==0.16.0\n",
    "RUN pip install pyarrow==14.0.2\n",
    "RUN pip install hyperopt\n",
    "RUN pip uninstall -y bson\n",
    "RUN pip install pymongo\n",
    "RUN pip install -q \"xgboost>=2.0,<3\"\n",
    "RUN pip install hydra-core \n",
    "RUN pip install --no-cache-dir -q \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\" \"pyopenssl<24\" \"cryptography<42\"\n",
    "```\n",
    "\n",
    "Make sure to add the **pluggable workspace tools**\n",
    "```\n",
    "jupyter:\n",
    "  title: \"Jupyter (Python, R, Julia)\"\n",
    "  iconUrl: \"/assets/images/workspace-logos/Jupyter.svg\"\n",
    "  start: [ \"/opt/domino/workspaces/jupyter/start\" ]\n",
    "  supportedFileExtensions: [ \".ipynb\" ]\n",
    "  httpProxy:\n",
    "    port: 8888\n",
    "    rewrite: false\n",
    "    internalPath: \"/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}\"\n",
    "    requireSubdomain: false\n",
    "jupyterlab:\n",
    "  title: \"JupyterLab\"\n",
    "  iconUrl: \"/assets/images/workspace-logos/jupyterlab.svg\"\n",
    "  start: [  \"/opt/domino/workspaces/jupyterlab/start\" ]\n",
    "  httpProxy:\n",
    "    internalPath: \"/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}\"\n",
    "    port: 8888\n",
    "    rewrite: false\n",
    "    requireSubdomain: false\n",
    "vscode:\n",
    "  title: \"vscode\"\n",
    "  iconUrl: \"/assets/images/workspace-logos/vscode.svg\"\n",
    "  start: [ \"/opt/domino/workspaces/vscode/start\" ]\n",
    "  httpProxy:\n",
    "    port: 8888\n",
    "    requireSubdomain: false\n",
    "rstudio:\n",
    "  title: \"RStudio\"\n",
    "  iconUrl: \"/assets/images/workspace-logos/Rstudio.svg\"\n",
    "  start: [ \"/opt/domino/workspaces/rstudio/start\" ]\n",
    "  httpProxy:\n",
    "    port: 8888\n",
    "    requireSubdomain: false\n",
    "```\n",
    "  \n",
    "\n",
    "\n",
    "## Ray cluster environment\n",
    "\n",
    "**Base Env** - `quay.io/domino/ray-cluster-environment:ray2.36.0-py3.10-domino6.0`\n",
    "\n",
    "**Dockerfile**\n",
    "\n",
    "```\n",
    "USER root\n",
    "RUN apt update && apt install -y unixodbc unixodbc-dev\n",
    "RUN /opt/conda/bin/pip install h11==0.16.0\n",
    "RUN pip install pyarrow==14.0.2\n",
    "RUN pip install hyperopt\n",
    "RUN pip uninstall -y bson\n",
    "RUN pip install pymongo\n",
    "RUN pip install -q \"xgboost>=2.0,<3\"\n",
    "RUN pip install hydra-core \n",
    "RUN pip install --no-cache-dir -q \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\" \"pyopenssl<24\" \"cryptography<42\"\n",
    "USER ubuntu    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f299857-3935-4e11-9083-7e8188fc693f",
   "metadata": {},
   "source": [
    "## Start the workspace\n",
    "\n",
    "Start a workspace with a Ray cluster with 3 medium sized worker nodes and 1 small sized head node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c011f-0686-4ab2-8aca-369d7806a9b0",
   "metadata": {},
   "source": [
    "## Pick the configuration file\n",
    "\n",
    "We use hydra to run various types of run (Root folder is `/mnt/code/conf`) -\n",
    "1. `config.yaml` - Default values\n",
    "2. `env/local.yaml` - Runs in Domino via Domino datasets as a shared location. Uses a small subset of the total data\n",
    "3. `env/dev.yaml` - Runs with S3 bucket and uses a small subset of the total data\n",
    "4. `env/test.yaml` - Runs with S3 bucket and uses the full dataset\n",
    "5. `env/prod.yaml` - Runs with S3 bucket and uses the full dataset\n",
    "\n",
    "There is no difference between `test` and `prod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a093c1ef-1f20-4e1b-84de-891d32ff48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_env=\"local\" #Picks the appropriate hydra config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336f048-16eb-441b-9c12-74309c613578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost as mlflow_xgb\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "from ray.data import read_parquet\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "try:\n",
    "    from ray.tune.callback import Callback      # Ray >= 2.6\n",
    "except ImportError:\n",
    "    from ray.tune.callbacks import Callback     # Older Ray\n",
    "from utils import ddl_cluster_scaling_client\n",
    "from utils import mlflow_utils\n",
    "from utils import ray_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129018e0-7750-40d8-8b4b-b0333bcc1d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fe473df-6ff9-4cce-8bbf-10f4493cb6dd",
   "metadata": {},
   "source": [
    "## Pre-requsites - When using S3 bucket\n",
    "\n",
    "Configure the following user environment variables\n",
    "\n",
    "1. AWS_ROLE_ARN - This is the AWS role being assumed via IR\n",
    "2. S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83111d82-fcdd-4b35-a491-f714007b3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify your role identity in AWS\n",
    "import boto3\n",
    "sts = boto3.client(\"sts\")\n",
    "identity = sts.get_caller_identity()\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb3e5a-835a-4942-9fbb-2bc685605ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.rename(columns={\"MedHouseVal\": \"median_house_value\"})\n",
    "\n",
    "# Split\n",
    "train, tmp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val, test  = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save locally\n",
    "train.to_parquet(\"/tmp/train.parquet\", index=False)\n",
    "val.to_parquet(\"/tmp/val.parquet\", index=False)\n",
    "test.to_parquet(\"/tmp/test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af9bda-73fe-4dde-b2b5-c9c6e54ee9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset and push to S3\n",
    "# Push to S3\n",
    "!aws s3 cp /tmp/train.parquet s3://${S3_BUCKET_NAME}/end-to-end/california/train/\n",
    "!aws s3 cp /tmp/val.parquet   s3://${S3_BUCKET_NAME}/end-to-end/california/val/\n",
    "!aws s3 cp /tmp/test.parquet  s3://${S3_BUCKET_NAME}/end-to-end/california/test/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1aa7ff-e653-40af-9e36-9faa8a0ff5b3",
   "metadata": {},
   "source": [
    "## Pre-requsites - When using a Domino dataset\n",
    "\n",
    "If you are working on your own project there will be a dataset with the same name as the project. Configure the env variable\n",
    "`DATASET_NAME` below to that dataset name. If not associate a dataset you have permissions to for your project and name the \n",
    "environment variable accordingly.\n",
    "\n",
    "Also note that the `ROOT_DOMINO_DATASET_FOLDER` is `/mnt/data` for a git backed project. For a Domino File System backed project it is `/domino/datasets/local/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ee63e-3378-4777-ad4b-0a0192a448d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DOMINO_DATASET_FOLDER = \"/mnt/data\" #For DFS based project use /domino/datasets/local/\n",
    "DATASET_NAME=\"ddl-end-to-end-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1897eb5-a71b-41b1-b8d9-2e85a0709eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file_with_dirs(src_file, dest_file):\n",
    "    \"\"\"\n",
    "    Copy a file from src_file to dest_file, creating destination\n",
    "    folders if they don't exist.\n",
    "    \"\"\"\n",
    "    # Ensure destination directory exists\n",
    "    dest_dir = os.path.dirname(dest_file)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    # Copy the file\n",
    "    shutil.copy2(src_file, dest_file)  # copy2 preserves metadata\n",
    "    print(f\"Copied {src_file} -> {dest_file}\")\n",
    "\n",
    "\n",
    "src = f\"/tmp/train.parquet\"\n",
    "dest = f\"{ROOT_DOMINO_DATASET_FOLDER}/{DATASET_NAME}/end-to-end/california/train\"\n",
    "copy_file_with_dirs(src, dest)\n",
    "\n",
    "src = f\"/tmp/val.parquet\"\n",
    "dest = f\"{ROOT_DOMINO_DATASET_FOLDER}/{DATASET_NAME}/end-to-end/california/val\"\n",
    "copy_file_with_dirs(src, dest)\n",
    "\n",
    "src = f\"/tmp/test.parquet\"\n",
    "dest = f\"{ROOT_DOMINO_DATASET_FOLDER}/{DATASET_NAME}/end-to-end/california/test\"\n",
    "copy_file_with_dirs(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adda697-3841-450e-98f4-bed31db79159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _s3p(root: str, sub: str) -> str:\n",
    "    \"\"\"Safe join for S3/posix URIs.\"\"\"\n",
    "    val = f\"{root.rstrip('/')}/{sub.lstrip('/')}\"\n",
    "    return val \n",
    "\n",
    "\n",
    "def read_parquet_to_pandas(uri: str, columns=None, limit: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust Parquet→pandas loader that bypasses Ray Data.\n",
    "    Works with local paths and s3:// (PyArrow uses AWS_* env vars / IRSA).\n",
    "    \"\"\"\n",
    "    ds = pds.dataset(uri.rstrip(\"/\"), format=\"parquet\")\n",
    "    if limit is None:\n",
    "        return ds.to_table(columns=columns).to_pandas()\n",
    "\n",
    "    # Respect limit across files/row groups\n",
    "    scanner = pds.Scanner.from_dataset(ds, columns=columns)\n",
    "    batches, rows = [], 0\n",
    "    for b in scanner.to_batches():\n",
    "        batches.append(b)\n",
    "        rows += len(b)\n",
    "        if rows >= limit:\n",
    "            return pa.Table.from_batches(batches)[:limit].to_pandas()\n",
    "    return pa.Table.from_batches(batches).to_pandas()\n",
    "\n",
    "\n",
    "def main(experiment_name:str,data_dir: str,\n",
    "         model_name:str,model_desc:str,\n",
    "         num_workers: int = 4, cpus_per_worker: int = 1,  DEV_FAST: bool = False):\n",
    "    \"\"\"\n",
    "    Quick knobs:\n",
    "      - num_workers * cpus_per_worker = CPUs per trial.\n",
    "      - trainer_resources={\"CPU\":0} so the driver doesn't steal a core.\n",
    "      - PACK placement to keep trials tight.\n",
    "      - max_concurrent_trials caps parallel trials.\n",
    "      - num_boost_round / early_stopping_rounds control trial length.\n",
    "      - nthread = cpus_per_worker to avoid oversubscription.\n",
    "    \"\"\"\n",
    "\n",
    "    exp_id = mlflow_utils.ensure_mlflow_experiment(experiment_name)\n",
    "    mv = mlflow_utils.ensure_registered_model(model_name)\n",
    "    # Storage: local for dev, S3/your env otherwise\n",
    "    RUN_STORAGE = os.environ.get(\"RAY_AIR_STORAGE\", f\"{data_dir}/air/xgb\")\n",
    "    TUNER_STORAGE = \"/tmp/air-dev\" if DEV_FAST else RUN_STORAGE\n",
    "    FINAL_STORAGE = \"/mnt/data/ddl-end-to-end-demo/air/final_fit\" if DEV_FAST else RUN_STORAGE\n",
    "\n",
    "    # Sanity: workers see IRSA env?\n",
    "    @ray.remote\n",
    "    def _peek():\n",
    "        import os\n",
    "        return {\n",
    "            \"ROLE\": bool(os.environ.get(\"AWS_ROLE_ARN\")),\n",
    "            \"TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\"),\n",
    "            \"REGION\": os.environ.get(\"AWS_REGION\"),\n",
    "        }\n",
    "    print(\"Worker env peek:\", ray.get(_peek.remote()))\n",
    "\n",
    "    # MLflow (experiment + parent run)\n",
    "    CLUSTER_TRACKING_URI = os.environ[\"CLUSTER_MLFLOW_TRACKING_URI\"]\n",
    "    \n",
    "    \n",
    "    client = MlflowClient()\n",
    "\n",
    "\n",
    "    parent = client.create_run(\n",
    "        experiment_id=exp_id,\n",
    "        tags={\"mlflow.runName\": \"xgb_parent\", \"role\": \"tune_parent\"},\n",
    "    )\n",
    "    parent_run_id = parent.info.run_id\n",
    "    print(\"Parent run id:\", parent_run_id)\n",
    "\n",
    "    # Data (Ray Datasets for training/val)\n",
    "    train_ds = read_parquet(_s3p(data_dir, \"train\"), parallelism=num_workers)\n",
    "    val_ds   = read_parquet(_s3p(data_dir, \"val\"),   parallelism=num_workers)\n",
    "    test_ds  = read_parquet(_s3p(data_dir, \"test\"),  parallelism=num_workers)\n",
    "    print(\"Schema:\", train_ds.schema())\n",
    "\n",
    "    # Label + features\n",
    "    label_col = \"median_house_value\"\n",
    "    feature_cols = [c for c in train_ds.schema().names if c != label_col]\n",
    "    keep = feature_cols + [label_col]\n",
    "    train_ds = train_ds.select_columns(keep)\n",
    "    val_ds   = val_ds.select_columns(keep)\n",
    "\n",
    "    # DEV: trim Ray Datasets used for training; eval will bypass Ray entirely\n",
    "    if DEV_FAST:\n",
    "        train_ds = train_ds.limit(5_000)\n",
    "        val_ds   = val_ds.limit(2_000)\n",
    "\n",
    "    # --- Build test DataFrame without Ray (avoids 'Global node is not initialized') ---\n",
    "    test_uri = _s3p(data_dir, \"test\")\n",
    "    test_pdf = read_parquet_to_pandas(\n",
    "        test_uri, columns=keep, limit=2_000 if DEV_FAST else None\n",
    "    )\n",
    "\n",
    "    # Search space\n",
    "    param_space = {\n",
    "        \"params\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": tune.loguniform(1e-3, 3e-1),\n",
    "            \"max_depth\": tune.randint(4, 12),\n",
    "            \"min_child_weight\": tune.loguniform(1e-2, 10),\n",
    "            \"subsample\": tune.uniform(0.6, 1.0),\n",
    "            \"colsample_bytree\": tune.uniform(0.6, 1.0),\n",
    "            \"lambda\": tune.loguniform(1e-3, 10),\n",
    "            \"alpha\": tune.loguniform(1e-3, 10),\n",
    "        },\n",
    "        \"num_boost_round\": 300,\n",
    "        \"early_stopping_rounds\": 20,\n",
    "    }\n",
    "\n",
    "    # Dev shortcuts\n",
    "    if DEV_FAST:\n",
    "        param_space[\"num_boost_round\"] = 20\n",
    "        param_space[\"early_stopping_rounds\"] = 5\n",
    "        num_workers = 1\n",
    "        cpus_per_worker = 1\n",
    "        NUM_SAMPLES = 5\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "    else:\n",
    "        NUM_SAMPLES = 30\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "\n",
    "    # Threads per worker\n",
    "    param_space[\"params\"][\"nthread\"] = cpus_per_worker\n",
    "    print(\"Per-trial CPUs =\", num_workers * cpus_per_worker)\n",
    "\n",
    "    # Scaling / placement\n",
    "    scaling = ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=False,\n",
    "        resources_per_worker={\"CPU\": cpus_per_worker},\n",
    "        trainer_resources={\"CPU\": 0},\n",
    "        placement_strategy=\"PACK\",\n",
    "    )\n",
    "\n",
    "    # Trainable\n",
    "    trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=param_space[\"params\"],\n",
    "        datasets={\"train\": train_ds, \"valid\": val_ds},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "    )\n",
    "\n",
    "    # Search + scheduler\n",
    "    MAX_T = int(param_space[\"num_boost_round\"])\n",
    "    GRACE = int(min(param_space.get(\"early_stopping_rounds\", 1), MAX_T))\n",
    "    algo = HyperOptSearch(metric=\"valid-rmse\", mode=\"min\")\n",
    "    scheduler = ASHAScheduler(max_t=MAX_T, grace_period=GRACE, reduction_factor=3)\n",
    "\n",
    "    # MLflow callback (child runs)\n",
    "    mlflow_cb = MLflowLoggerCallback(\n",
    "        tracking_uri=CLUSTER_TRACKING_URI,\n",
    "        experiment_name=experiment_name,\n",
    "        save_artifact=SAVE_ARTIFACTS,\n",
    "        tags={\"mlflow.parentRunId\": parent_run_id},\n",
    "    )\n",
    "\n",
    "    # Tuner\n",
    "    tuner = tune.Tuner(\n",
    "        trainer.as_trainable(),\n",
    "        run_config=RunConfig(\n",
    "            name=\"xgb_from_s3_irsa\",\n",
    "            storage_path=TUNER_STORAGE,\n",
    "            callbacks=[mlflow_cb],\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            metric=\"valid-rmse\",\n",
    "            mode=\"min\",\n",
    "            num_samples=NUM_SAMPLES,\n",
    "            max_concurrent_trials=MAX_CONCURRENT,\n",
    "        ),\n",
    "        param_space={\"params\": param_space[\"params\"]},\n",
    "    )\n",
    "\n",
    "    # Tune\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"valid-rmse\", mode=\"min\")\n",
    "    print(\"Best config:\", best.config)\n",
    "    print(\"Best valid RMSE:\", best.metrics.get(\"valid-rmse\"))\n",
    "\n",
    "    # Final fit (train + val)\n",
    "    merged = train_ds.union(val_ds)\n",
    "    final_trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=best.config[\"params\"],\n",
    "        datasets={\"train\": merged},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "        run_config=RunConfig(name=\"final_fit\", storage_path=FINAL_STORAGE),\n",
    "    )\n",
    "    final_result = final_trainer.fit()\n",
    "    final_ckpt = final_result.checkpoint\n",
    "\n",
    "    # Load Booster from checkpoint\n",
    "    with final_ckpt.as_directory() as ckpt_dir:\n",
    "        print(\"Checkpoint dir:\", ckpt_dir, \"files:\", os.listdir(ckpt_dir))\n",
    "        candidates = [\"model.json\", \"model.ubj\", \"model.xgb\", \"xgboost_model.json\", \"model\"]\n",
    "        model_path = next(\n",
    "            (os.path.join(ckpt_dir, f) for f in candidates if os.path.exists(os.path.join(ckpt_dir, f))),\n",
    "            None,\n",
    "        )\n",
    "        if not model_path:\n",
    "            raise FileNotFoundError(f\"No XGBoost model file found in checkpoint dir: {ckpt_dir}\")\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(model_path)\n",
    "\n",
    "    # Driver-side eval (no Ray dependency)\n",
    "    X_test = test_pdf.drop(columns=[label_col])\n",
    "    \n",
    "    dmat = xgb.DMatrix(X_test)\n",
    "    y_pred = booster.predict(dmat)\n",
    "    rmse = math.sqrt(((test_pdf[label_col].to_numpy() - y_pred) ** 2).mean())\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    \n",
    "    # Log final under parent\n",
    "\n",
    "    with mlflow.start_run(run_id=parent_run_id):\n",
    "        X_example = X_test.head(5).copy()  \n",
    "        y_example = booster.predict(xgb.DMatrix(X_example))\n",
    "        sig = infer_signature(X_example, y_example)\n",
    "        with mlflow.start_run(run_name=\"final_fit\", nested=True) as final_run:\n",
    "            mlflow.log_params(best.config.get(\"params\", {}))\n",
    "            mlflow.log_dict({\"label\": label_col, \"features\": feature_cols}, \"features.json\")\n",
    "            mlflow.log_metric(\"valid_rmse_best\", float(best.metrics.get(\"valid-rmse\")))\n",
    "            mlflow.log_metric(\"test_rmse\", float(rmse))\n",
    "            model_info = mlflow_xgb.log_model(booster, artifact_path=\"model\",signature=sig,input_example=X_example)\n",
    "\n",
    "            mv = mlflow_utils.register_model_version(model_name=model_name,model_desc=model_desc,\n",
    "                                                model_info=model_info,run=final_run)\n",
    "            \n",
    "            print(\"Name: {}\".format(mv.name))\n",
    "            print(\"Version: {}\".format(mv.version))\n",
    "            print(\"Description: {}\".format(mv.description))\n",
    "            print(\"Status: {}\".format(mv.status))\n",
    "            print(\"Stage: {}\".format(mv.current_stage))\n",
    "            \n",
    "    \n",
    "    run = client.get_run(parent_run_id)\n",
    "    if run.info.status == \"RUNNING\":\n",
    "        client.set_terminated(parent_run_id, \"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149bc2b-690e-4cc7-8408-225f28f2b55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522d26e-854b-43f2-b874-ecd7be87d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_env=\"local\" #Picks the appropriate hydra config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0c61b-69d8-4686-858b-67ad14a2d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Conf from Hydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "# Point Hydra to your conf/ directory\n",
    "with initialize(config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config\", overrides=[f\"env={which_env}\"])\n",
    "    #print(f\"Running in {cfg.env} environment\")\n",
    "    #print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "    \n",
    "    app_name = cfg.app.name\n",
    "\n",
    "    #For dev we use datasets\n",
    "    data_dir = cfg.app.data_dir\n",
    "    experiment_name = cfg.mlflow.experiment_name    \n",
    "    model_name = cfg.mlflow.model_name    \n",
    "    model_desc = cfg.mlflow.model_desc\n",
    "    ray_workers = cfg.env.ray.num_workers\n",
    "    cpus_per_worker = cfg.env.ray.cpus_per_worker\n",
    "    dev_fast = cfg.env.ray.dev_fast\n",
    "    #print(ray_workers)\n",
    "    #print(dev_fast)\n",
    "    \n",
    "# Disable tensorboard integration\n",
    "os.environ[\"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ba8ff-d2f0-4ad2-83b8-501b4dc41640",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All AWS Env variables are redundant when using locally\n",
    "\n",
    "RAY_JOB_ENV = {\n",
    "    \"AWS_ROLE_ARN\": os.environ.get(\"AWS_ROLE_ARN\", \"\"),\n",
    "    \"AWS_WEB_IDENTITY_TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\", \"\"),\n",
    "    \"AWS_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"AWS_DEFAULT_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\":\"1\",\n",
    "    \"TUNE_RESULT_BUFFER_LENGTH\": \"16\",\n",
    "    \"TUNE_RESULT_BUFFER_FLUSH_INTERVAL_S\": \"3\",    \n",
    "    \n",
    "}\n",
    "ray.shutdown()\n",
    "ray_utils.ensure_ray_connected(RAY_JOB_ENV,ray_ns=app_name)\n",
    "\n",
    "main(experiment_name=experiment_name,data_dir=data_dir, \n",
    "     model_name=model_name,model_desc=model_desc,\n",
    "     num_workers=ray_workers, cpus_per_worker=cpus_per_worker,DEV_FAST=dev_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7b117-675f-450b-8076-fcef8bf7771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mlflow_utils\n",
    "import pandas as pd\n",
    "import mlflow.pyfunc\n",
    "\n",
    "my_model = mlflow_utils.load_registered_model_version(model_name,\"latest\")\n",
    "\n",
    "# your split-style payload\n",
    "split = {\n",
    "  \"columns\": [\"MedInc\",\"HouseAge\",\"AveRooms\",\"AveBedrms\",\"Population\",\"AveOccup\",\"Latitude\",\"Longitude\"],\n",
    "  \"data\": [\n",
    "    [3.1333,30.0,5.925531914893617,1.1312056737588652,966.0,3.425531914893617,36.51,-119.65],\n",
    "    [2.3355,18.0,5.711722488038277,1.0598086124401913,1868.0,2.2344497607655502,33.97,-117.01],\n",
    "    [3.3669,29.0,4.5898778359511345,1.0767888307155322,1071.0,1.869109947643979,34.15,-118.37],\n",
    "    [3.875,46.0,4.0,1.0,59.0,4.538461538461538,33.12,-117.11],\n",
    "    [4.3482,9.0,5.7924528301886795,1.1037735849056605,409.0,1.929245283018868,35.36,-119.06]\n",
    "  ]\n",
    "}\n",
    "\n",
    "# make a DataFrame\n",
    "X = pd.DataFrame(split[\"data\"], columns=split[\"columns\"])\n",
    "preds = my_model.predict(X)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e00d6-0f97-4180-a994-7cb7fc0e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, sys, pyarrow as pa, pandas as pd\n",
    "print(\"DRIVER:\", sys.version)\n",
    "print(\"DRIVER pyarrow:\", pa.__version__)\n",
    "print(\"DRIVER pandas :\", pd.__version__)\n",
    "\n",
    "@ray.remote\n",
    "def _env_probe():\n",
    "    import sys, pyarrow as pa, pandas as pd\n",
    "    return {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"pyarrow\": pa.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "    }\n",
    "\n",
    "print(\"WORKER:\", ray.get(_env_probe.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd9c4b-94c5-44b8-9a58-51cbfbc16361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
