{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0a54bc1-58ed-42ae-ae0c-341a0ca2d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1336f048-16eb-441b-9c12-74309c613578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import ray\n",
    "from ray import air, train\n",
    "from ray.train import Checkpoint\n",
    "from ray.train.torch import TorchTrainer, get_device, prepare_model, prepare_data_loader\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "\n",
    "try:\n",
    "    from ray.tune.callback import Callback      # Ray >= 2.6\n",
    "except ImportError:\n",
    "    from ray.tune.callbacks import Callback     # Older Ray\n",
    "from utils import ddl_cluster_scaling_client\n",
    "from utils import mlflow_utils\n",
    "from utils import ray_utils\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2c7b117-675f-450b-8076-fcef8bf7771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import json\n",
    "import ray\n",
    "\n",
    "# ----- RUN CONFIG (edit as needed) -----\n",
    "\n",
    "'''\n",
    "#Pip installs do not work yet in Domino. Add the libraries to your environment or install using command line in the worker\n",
    "RUN_CONFIG = {\n",
    "    # Packages are installed into an isolated env for each worker by Ray\n",
    "    \n",
    "    \"pip\": [\n",
    "        \"ray[train]==2.49.1\",\n",
    "        \"torch==2.3.1\",\n",
    "        \"torchvision==0.18.1\",\n",
    "        \"torchaudio==2.3.1\"\n",
    "    ],\n",
    "    # Environment variables to expose inside workers\n",
    "    \"env_vars\": {\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",     # harmless on CPU-only boxes; useful hint on GPU clusters without IB/EFA\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\"\n",
    "    }\n",
    "}\n",
    "'''\n",
    "RUN_CONFIG = {\n",
    "    # Packages are installed into an isolated env for each worker by Ray\n",
    "    # Environment variables to expose inside workers\n",
    "    \"env_vars\": {\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",     # harmless on CPU-only boxes; useful hint on GPU clusters without IB/EFA\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae4e00d6-0f97-4180-a994-7cb7fc0e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6ce6fe0-a01c-410d-9b65-fd2843dcf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U \"pyopenssl<24\" \"cryptography<42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0a23211-b6aa-469e-878f-b19ec0943134",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "def probe_worker(env_keys):\n",
    "    import os, platform, importlib\n",
    "    from ray.runtime_context import get_runtime_context\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "\n",
    "    def _id(ctx, name):\n",
    "        if not hasattr(ctx, name):\n",
    "            return None\n",
    "        v = getattr(ctx, name)()  # may be bytes-like w/ .hex() or already a str\n",
    "        try:\n",
    "            return v.hex()\n",
    "        except AttributeError:\n",
    "            return str(v)\n",
    "\n",
    "    # Torch proof without leaking torch objects in the return\n",
    "    torch = importlib.import_module(\"torch\")\n",
    "    x = torch.tensor([1.0, 2.0, 3.0]) * 2.0\n",
    "    sample_sum = float(x.sum().item())\n",
    "    torch_version = str(getattr(torch, \"__version__\", \"unknown\"))\n",
    "    cuda_avail = bool(hasattr(torch, \"cuda\") and torch.cuda.is_available())\n",
    "    del x, torch\n",
    "\n",
    "    return {\n",
    "        \"node\": platform.node(),\n",
    "        \"pid\": os.getpid(),\n",
    "        \"python_executable\": os.sys.executable,\n",
    "        \"torch_version\": torch_version,\n",
    "        \"torch_cuda_available\": cuda_avail,\n",
    "        \"env\": {k: os.environ.get(k) for k in env_keys},\n",
    "        \"torch_sample_sum\": sample_sum,\n",
    "        \"ids\": {\n",
    "            \"task_id\": _id(ctx, \"get_task_id\"),\n",
    "            \"actor_id\": _id(ctx, \"get_actor_id\"),\n",
    "            \"node_id\": _id(ctx, \"get_node_id\"),\n",
    "            \"job_id\": _id(ctx, \"get_job_id\"),\n",
    "            \"namespace\": ctx.get_namespace() if hasattr(ctx, \"get_namespace\") else None,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baa8252e-696f-4b3d-90c9-35695188045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 17:30:34,920\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n",
      "2025-09-20 17:30:35,945\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"worker_1\": {\n",
      "    \"wrote\": \"/mnt/data/ddl-end-to-end-demo/1f000000/81a4966da5e2d352ffffffffffffffffffffffff1f000000/probe_1f000000_81a4966da5e2d352ffffffffffffffffffffffff1f000000_157.json\",\n",
      "    \"size\": 17165\n",
      "  },\n",
      "  \"worker_2\": {\n",
      "    \"wrote\": \"/mnt/data/ddl-end-to-end-demo/1f000000/ebd77e3d348811d7ffffffffffffffffffffffff1f000000/probe_1f000000_ebd77e3d348811d7ffffffffffffffffffffffff1f000000_156.json\",\n",
      "    \"size\": 17165\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# file: simple_ray_runtime_env_fix.py\n",
    "import os, json, platform\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "SHARED_DIR = \"/mnt/data/ddl-end-to-end-demo\"\n",
    "\n",
    "RUNTIME_ENV = {\n",
    "    \"env_vars\": {\n",
    "        \"SHARED_DIR\": SHARED_DIR,\n",
    "        \"APP_MODE\": \"probe\",\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "    }\n",
    "}\n",
    "\n",
    "if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "   addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "   ray.shutdown()\n",
    "   ray.init(\n",
    "      address=addr or \"auto\",\n",
    "      runtime_env=RUNTIME_ENV,\n",
    "      namespace=\"demo-ray-ns\"\n",
    "  )\n",
    "\n",
    "def _norm_id(val):\n",
    "    try:\n",
    "        return val.hex()\n",
    "    except Exception:\n",
    "        return str(val)\n",
    "\n",
    "@ray.remote(num_cpus=1)\n",
    "def probe_worker():\n",
    "    import os, time, socket, json, tempfile\n",
    "    from ray.runtime_context import get_runtime_context\n",
    "    from pathlib import Path\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "    job_id  = _norm_id(ctx.get_job_id())  if hasattr(ctx, \"get_job_id\") else \"na\"\n",
    "    task_id = _norm_id(ctx.get_task_id()) if hasattr(ctx, \"get_task_id\") else \"na\"\n",
    "\n",
    "    shared = Path(os.environ[\"SHARED_DIR\"],job_id,task_id)  # pulled from runtime_env\n",
    "    shared.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    payload = {\n",
    "        \"node\": platform.node(),\n",
    "        \"pid\": os.getpid(),\n",
    "        \"job_id\": job_id,\n",
    "        \"task_id\": task_id,\n",
    "        \"env\": dict(os.environ),\n",
    "        \"ts\": time.time(),\n",
    "    }\n",
    "\n",
    "    fname = f\"probe_{job_id}_{task_id}_{os.getpid()}.json\"\n",
    "    dest = shared / fname\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=str(shared)) as tmp:\n",
    "        json.dump(payload, tmp, indent=2)\n",
    "        tmp.flush()\n",
    "        os.fsync(tmp.fileno())\n",
    "        tmp_path = tmp.name\n",
    "    os.replace(tmp_path, dest)\n",
    "\n",
    "    return {\"wrote\": str(dest), \"size\": dest.stat().st_size}\n",
    "\n",
    "def main():\n",
    "    # --------------------------------------\n",
    "    if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "       addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "       ray.shutdown()\n",
    "       ray.init(\n",
    "          address=addr or \"auto\",\n",
    "          #runtime_env={\"env_vars\": RUNTIME_ENV},   # same env you used earlier\n",
    "          namespace=\"demo-ray-ns\"\n",
    "      )\n",
    "# Connect to Domino Ray if available; otherw\n",
    "    t1 = probe_worker.options(runtime_env=RUNTIME_ENV).remote()\n",
    "    t2 = probe_worker.options(runtime_env=RUNTIME_ENV).remote()\n",
    "    out1, out2 = ray.get([t1, t2])\n",
    "    print(json.dumps({\"worker_1\": out1, \"worker_2\": out2}, indent=2))\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d995288e-e1aa-4b82-ad68-ad8246d9b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mnist(data_root: Path):\n",
    "    data_root.mkdir(parents=True, exist_ok=True)\n",
    "    # Uses torchvision's built-in downloader/extractor\n",
    "    from torchvision import datasets, transforms\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "    datasets.MNIST(str(data_root), train=True,  download=True, transform=tfm)\n",
    "    datasets.MNIST(str(data_root), train=False, download=True, transform=tfm)\n",
    "\n",
    "\n",
    "data_dir = Path(\"/mnt/data/ddl-end-to-end-demo/mnist/\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "prepare_mnist(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832f5bf-f89a-4d61-8c38-47ae9302369d",
   "metadata": {},
   "source": [
    "## Apply Domsed mutation if using this under Istio\n",
    "\n",
    "Worker nodes need to be able to communicate over ephemeral ports for distributed training to work. When using Domino with istio \n",
    "you need to open a port (Ex. 29000) for the workers for both inbound and outbound connections. The `allow-custom-port-inbound-ray-interworker-comm` simply adds 29000 to the list of the `includeInboundPorts`\n",
    "\n",
    "Mutation to allow inbound to port `29000` - **allow-custom-port-inbound-ray-interworker-comm**\n",
    "\n",
    "```\n",
    "apiVersion: apps.dominodatalab.com/v1alpha1\n",
    "kind: Mutation\n",
    "metadata:\n",
    "  name: allow-custom-port-inbound-ray-interworker-comm\n",
    "  namespace: domino-platform\n",
    "rules:\n",
    "-\n",
    "  modifyAnnotation:\n",
    "    key: \"traffic.sidecar.istio.io/includeInboundPorts\"\n",
    "    value: \"2384,2385,11000,11001,11002,11003,11004,11005,11006,11007,11008,11009,11010,11011,11012,11013,11014,11015,11016,11017,11018,11019,11020,11021,11022,11023,11024,11025,11026,11027,11028,11029,11030,11031,11032,11033,11034,11035,11036,11037,11038,11039,11040,11041,11042,11043,11044,11045,11046,11047,11048,11049,11050,11051,11052,11053,11054,11055,11056,11057,11058,11059,11060,11061,11062,11063,11064,11065,11066,11067,11068,11069,11070,11071,11072,11073,11074,11075,11076,11077,11078,11079,11080,11081,11082,11083,11084,11085,11086,11087,11088,11089,11090,11091,11092,11093,11094,11095,11096,11097,11098,11099,29000\"\n",
    "\n",
    "```\n",
    "\n",
    "Mutation to allow outbound to port `29000` - **allow-custom-port-outbound-ray-interworker-comm**\n",
    "```\n",
    "apiVersion: apps.dominodatalab.com/v1alpha1\n",
    "kind: Mutation\n",
    "metadata:\n",
    "  name: allow-custom-port-outbound-ray-interworker-comm\n",
    "  namespace: domino-platform\n",
    "rules:\n",
    "-\n",
    "  modifyAnnotation:\n",
    "    key: \"traffic.sidecar.istio.io/excludeOutboundPorts\"\n",
    "    value: \"29000\"\n",
    "```\n",
    "\n",
    "When using without Istio you do not need these mutation. You also do not have to pass the env variable `\"MASTER_PORT\": \"29000\"`.\n",
    "Ray runtime will pick any ephemeral port to communicate for the purpose of distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc7cd3-3353-4aca-9213-4dc39b7ff1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ray\n",
    "from ray.air.config import RunConfig\n",
    "from ray.tune.logger import CSVLoggerCallback, JsonLoggerCallback\n",
    "from ray.runtime_context import get_runtime_context\n",
    "from pathlib import Path\n",
    "\n",
    "def build_model(num_classes: int = 10) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 512), nn.ReLU(),\n",
    "        nn.Linear(512, 256), nn.ReLU(),\n",
    "        nn.Linear(256, num_classes),\n",
    "    )\n",
    "    \n",
    "def train_loop_per_worker(config):\n",
    "    device = get_device()\n",
    "    model = prepare_model(build_model().to(device))\n",
    "\n",
    "    data_root = os.environ[\"SHARED_DIR\"]  # already populated\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # No network access in workers; just read the files\n",
    "    train_ds = datasets.MNIST(data_root, train=True,  download=False, transform=tfm)\n",
    "    test_ds  = datasets.MNIST(data_root, train=False, download=False, transform=tfm)\n",
    "\n",
    "    # Start conservative; you can raise num_workers/pin_memory after it works\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                              num_workers=0, pin_memory=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=512, shuffle=False,\n",
    "                              num_workers=0, pin_memory=False)\n",
    "\n",
    "    train_loader = prepare_data_loader(train_loader)\n",
    "    test_loader  = prepare_data_loader(test_loader)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = model(x).argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.numel()\n",
    "        acc = correct / total\n",
    "        train.report({\"epoch\": epoch, \"train_loss\": running, \"val_acc\": acc})\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    RUNTIME_ENV = {\n",
    "        \"env_vars\": {\n",
    "        \"GLOO_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"SHARED_DIR\": str(data_dir),\n",
    "        \"TUNE_DISABLE_AUTO_CALLBACKS\": \"1\",\n",
    "        \"TORCH_DISABLE_ADDR2LINE\": \"1\",     # stop symbolizer hang\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "        #\"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"NCCL_P2P_DISABLE\": \"1\",\n",
    "        \"NCCL_SHM_DISABLE\": \"1\",\n",
    "        \"OMP_NUM_THREADS\": \"2\",\n",
    "        # >>> Key bits for DDP rendezvous <<<\n",
    "        \"MASTER_PORT\": \"29000\",           # fixed, not ephemeral\n",
    "        \"GLOO_SOCKET_IFNAME\": \"eth0\",     # bind on pod interface\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",     # harmless even if CPU-only\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    # --------------------------------------\n",
    "    if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "       addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "       ray.shutdown()\n",
    "       ray.init(\n",
    "          address=addr or \"auto\",\n",
    "          runtime_env=RUNTIME_ENV,   # same env you used earlier\n",
    "          namespace=\"demo-ray-ns\"\n",
    "      )\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "    try:\n",
    "        job_id_hex = ctx.get_job_id().hex()\n",
    "    except Exception:\n",
    "        job_id_hex = \"unknown_job\"\n",
    "\n",
    "    DATASET_FOLDER = \"/mnt/data/ddl-end-to-end-demo/\"\n",
    "    shared = Path(DATASET_FOLDER,job_id_hex,\"ray_results\")  # pulled from runtime_env\n",
    "    shared.mkdir(parents=True, exist_ok=True)\n",
    "    STORAGE_PATH=str(shared)\n",
    "    \n",
    "    storage_base = Path(\"/mnt/data/ddl-end-to-end-demo\")  # head-visible shared\n",
    "    job_id_hex = getattr(ray.get_runtime_context(), \"get_job_id\", lambda: \"unknown\")()\n",
    "    job_id_hex = job_id_hex.hex() if hasattr(job_id_hex, \"hex\") else str(job_id_hex)\n",
    "    storage_path = str(storage_base / job_id_hex / \"ray_results\")\n",
    "    \n",
    "    os.environ[\"TUNE_DISABLE_AUTO_CALLBACKS\"] = \"1\"\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker,\n",
    "        train_loop_config={\"lr\": 1e-3, \"batch_size\": 256, \"epochs\": 5},\n",
    "        scaling_config=ScalingConfig(\n",
    "            num_workers=2,\n",
    "            use_gpu=True,                      # keep CPU+gloo until stable\n",
    "            resources_per_worker={\"CPU\": 2,\"GPU\": 1},\n",
    "            trainer_resources={\"CPU\": 0},               # <— key change\n",
    "            placement_strategy=\"SPREAD\",     \n",
    "            #placement_strategy=\"PACK\",          # single-node to avoid networking issues\n",
    "        ),\n",
    "        run_config=RunConfig(\n",
    "            name=f\"mnist_torch_ddp_{job_id_hex}\",\n",
    "            storage_path=STORAGE_PATH,\n",
    "            callbacks=[CSVLoggerCallback(), JsonLoggerCallback()],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eff962e3-ae37-4083-8517-dd95b76b9420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m View detailed results here: /mnt/data/ddl-end-to-end-demo/unknown_job/ray_results/mnist_torch_ddp_20000000\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-09-20_07-15-22_475265_1/artifacts/2025-09-20_10-30-43/mnist_torch_ddp_20000000/driver_artifacts`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m Training started with configuration:\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╭──────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ Training config                      │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ├──────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ train_loop_config/batch_size     256 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ train_loop_config/epochs           5 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ train_loop_config/lr           0.001 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╰──────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=325, ip=100.64.60.227)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TorchTrainer pid=280, ip=100.64.60.227)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=280, ip=100.64.60.227)\u001b[0m - (node_id=3b69fc3f37ca82c7ec924c97f63485d15dba64e05e8e9b299ded9f91, ip=100.64.60.227, pid=325) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=280, ip=100.64.60.227)\u001b[0m - (node_id=135baf60627c984443801299c5cae9747aa01920cc1d82b8701a2bd6, ip=100.64.21.58, pid=156) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(RayTrainWorker pid=325, ip=100.64.60.227)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=325, ip=100.64.60.227)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "\u001b[36m(RayTrainWorker pid=156, ip=100.64.21.58)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=156, ip=100.64.21.58)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m Training finished iteration 1 at 2025-09-20 10:31:01. Total running time: 16s\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_this_iter_s      11.6474 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_total_s          11.6474 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ training_iteration          1 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ epoch                       0 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ train_loss            55.5477 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ val_acc                0.9386 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m Training finished iteration 2 at 2025-09-20 10:31:07. Total running time: 23s\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_this_iter_s      6.17051 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_total_s          17.8179 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ training_iteration          2 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ epoch                       1 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ train_loss            20.1981 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ val_acc                0.9558 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m Training finished iteration 3 at 2025-09-20 10:31:13. Total running time: 29s\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_this_iter_s      6.08297 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_total_s          23.9009 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ training_iteration          3 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ epoch                       2 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ train_loss            13.4598 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ val_acc                 0.966 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m Training finished iteration 4 at 2025-09-20 10:31:19. Total running time: 35s\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_this_iter_s      5.91588 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_total_s          29.8168 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ training_iteration          4 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ epoch                       3 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ train_loss            9.90304 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ val_acc                0.9708 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m Training finished iteration 5 at 2025-09-20 10:31:25. Total running time: 41s\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╭───────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ Training result               │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ├───────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ checkpoint_dir_name           │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_this_iter_s      6.02518 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ time_total_s           35.842 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ training_iteration          5 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ epoch                       4 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ train_loss            7.54044 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m │ val_acc                0.9724 │\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m ╰───────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m Training completed after 5 iterations at 2025-09-20 10:31:26. Total running time: 42s\n",
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=11812)\u001b[0m Wrote the latest version of all result files and experiment state to '/mnt/data/ddl-end-to-end-demo/unknown_job/ray_results/mnist_torch_ddp_20000000' in 0.0226s.\n"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea18d3-8956-48b7-84fe-5efab74508dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45367d66-4f56-4e1a-98b0-44504bd70657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
