{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a54bc1-58ed-42ae-ae0c-341a0ca2d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1336f048-16eb-441b-9c12-74309c613578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 22:27:11,170\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-19 22:27:11,250\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-09-19 22:27:13,045\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import ray\n",
    "from ray import air, train\n",
    "from ray.train import Checkpoint\n",
    "from ray.train.torch import TorchTrainer, get_device, prepare_model, prepare_data_loader\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "\n",
    "try:\n",
    "    from ray.tune.callback import Callback      # Ray >= 2.6\n",
    "except ImportError:\n",
    "    from ray.tune.callbacks import Callback     # Older Ray\n",
    "from utils import ddl_cluster_scaling_client\n",
    "from utils import mlflow_utils\n",
    "from utils import ray_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe473df-6ff9-4cce-8bbf-10f4493cb6dd",
   "metadata": {},
   "source": [
    "## Pre-requsites\n",
    "\n",
    "Configure the following user environment variables\n",
    "\n",
    "1. AWS_ROLE_ARN - This is the AWS role being assumed via IR\n",
    "2. S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8af9bda-73fe-4dde-b2b5-c9c6e54ee9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../tmp/train.parquet to s3://ddl-wadkars/end-to-end/california/train/train.parquet\n",
      "upload: ../../../tmp/val.parquet to s3://ddl-wadkars/end-to-end/california/val/val.parquet\n",
      "upload: ../../../tmp/test.parquet to s3://ddl-wadkars/end-to-end/california/test/test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download dataset and push to S3\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.rename(columns={\"MedHouseVal\": \"median_house_value\"})\n",
    "\n",
    "# Split\n",
    "train, tmp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val, test  = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save locally\n",
    "train.to_parquet(\"/tmp/train.parquet\", index=False)\n",
    "val.to_parquet(\"/tmp/val.parquet\", index=False)\n",
    "test.to_parquet(\"/tmp/test.parquet\", index=False)\n",
    "\n",
    "# Push to S3\n",
    "!aws s3 cp /tmp/train.parquet s3://${S3_BUCKET_NAME}/end-to-end/california/train/\n",
    "!aws s3 cp /tmp/val.parquet   s3://${S3_BUCKET_NAME}/end-to-end/california/val/\n",
    "!aws s3 cp /tmp/test.parquet  s3://${S3_BUCKET_NAME}/end-to-end/california/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7040bd1-cd6e-4be0-98d9-2f90d4a527ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kind = \"rayclusters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ca4926-350b-4256-9377-6fda5f7e8d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Status code 200\n",
      "ray-68cdcd8314e2171c9b0f2508\n",
      "http://ddl-cluster-scaler-svc.domino-field.svc.cluster.local/ddl_cluster_scaler/cluster/rayclusters/ray-68cdcd8314e2171c9b0f2508\n",
      "Status code 200\n",
      "Expected worker nodes 2\n",
      "Current worker nodes ['ray-68cdcd8314e2171c9b0f2508-ray-worker-0', 'ray-68cdcd8314e2171c9b0f2508-ray-worker-1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = ddl_cluster_scaling_client.scale_cluster(cluster_kind=cluster_kind,head_hw_tier_name=\"Medium\", worker_hw_tier_name=\"Medium\", replicas=2)\n",
    "json.dumps(j, indent=2, sort_keys=True, ensure_ascii=False)\n",
    "ddl_cluster_scaling_client.wait_until_scaling_complete(cluster_kind=cluster_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55bb35-622c-48cc-8204-fc219791c577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab02e5-1bff-4223-a424-8a1a78496bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ddl_cluster_scaling_client\n",
    "j = ddl_cluster_scaling_client.restart_head_node(cluster_kind=\"rayclusters\")\n",
    "restarts_at = j['started_at']\n",
    "print(restarts_at)\n",
    "ddl_cluster_scaling_client.restart_head_node_status(cluster_kind=\"rayclusters\",restarted_since=restarts_at)\n",
    "ddl_cluster_scaling_client.wait_until_node_restarted(cluster_kind=\"rayclusters\",restarted_since=restarts_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c7b117-675f-450b-8076-fcef8bf7771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import json\n",
    "import ray\n",
    "\n",
    "# ----- RUN CONFIG (edit as needed) -----\n",
    "\n",
    "'''\n",
    "#Pip installs do not work yet in Domino. Add the libraries to your environment or install using command line in the worker\n",
    "RUN_CONFIG = {\n",
    "    # Packages are installed into an isolated env for each worker by Ray\n",
    "    \n",
    "    \"pip\": [\n",
    "        \"ray[train]==2.49.1\",\n",
    "        \"torch==2.3.1\",\n",
    "        \"torchvision==0.18.1\",\n",
    "        \"torchaudio==2.3.1\"\n",
    "    ],\n",
    "    # Environment variables to expose inside workers\n",
    "    \"env_vars\": {\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",     # harmless on CPU-only boxes; useful hint on GPU clusters without IB/EFA\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\"\n",
    "    }\n",
    "}\n",
    "'''\n",
    "RUN_CONFIG = {\n",
    "    # Packages are installed into an isolated env for each worker by Ray\n",
    "    # Environment variables to expose inside workers\n",
    "    \"env_vars\": {\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",     # harmless on CPU-only boxes; useful hint on GPU clusters without IB/EFA\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e00d6-0f97-4180-a994-7cb7fc0e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce6fe0-a01c-410d-9b65-fd2843dcf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"pyopenssl<24\" \"cryptography<42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a23211-b6aa-469e-878f-b19ec0943134",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "def probe_worker(env_keys):\n",
    "    import os, platform, importlib\n",
    "    from ray.runtime_context import get_runtime_context\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "\n",
    "    def _id(ctx, name):\n",
    "        if not hasattr(ctx, name):\n",
    "            return None\n",
    "        v = getattr(ctx, name)()  # may be bytes-like w/ .hex() or already a str\n",
    "        try:\n",
    "            return v.hex()\n",
    "        except AttributeError:\n",
    "            return str(v)\n",
    "\n",
    "    # Torch proof without leaking torch objects in the return\n",
    "    torch = importlib.import_module(\"torch\")\n",
    "    x = torch.tensor([1.0, 2.0, 3.0]) * 2.0\n",
    "    sample_sum = float(x.sum().item())\n",
    "    torch_version = str(getattr(torch, \"__version__\", \"unknown\"))\n",
    "    cuda_avail = bool(hasattr(torch, \"cuda\") and torch.cuda.is_available())\n",
    "    del x, torch\n",
    "\n",
    "    return {\n",
    "        \"node\": platform.node(),\n",
    "        \"pid\": os.getpid(),\n",
    "        \"python_executable\": os.sys.executable,\n",
    "        \"torch_version\": torch_version,\n",
    "        \"torch_cuda_available\": cuda_avail,\n",
    "        \"env\": {k: os.environ.get(k) for k in env_keys},\n",
    "        \"torch_sample_sum\": sample_sum,\n",
    "        \"ids\": {\n",
    "            \"task_id\": _id(ctx, \"get_task_id\"),\n",
    "            \"actor_id\": _id(ctx, \"get_actor_id\"),\n",
    "            \"node_id\": _id(ctx, \"get_node_id\"),\n",
    "            \"job_id\": _id(ctx, \"get_job_id\"),\n",
    "            \"namespace\": ctx.get_namespace() if hasattr(ctx, \"get_namespace\") else None,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10dd3a8b-6669-45fa-9368-dc722d74746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 22:27:29,641\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    }
   ],
   "source": [
    "# file: simple_ray_runtime_env_fix.py\n",
    "import os, json, platform\n",
    "import ray\n",
    "import torch\n",
    "# ---- hard fix for your error: avoid virtualenv, use stdlib venv ----\n",
    "os.environ[\"RAY_RUNTIME_ENV_VENV_CREATION\"] = \"venv\"  # must be set before ray.init\n",
    "\n",
    "# ---- choose one path ----\n",
    "USE_PREINSTALLED = False  # True = skip installs; all deps must already be on nodes\n",
    "\n",
    "RUNTIME_ENV = {\n",
    "    \"env_vars\": {\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "    },\n",
    "    \"eager_install\": True,  # fail fast during setup\n",
    "}\n",
    "\n",
    "if not USE_PREINSTALLED:\n",
    "    # Ray will create a venv using stdlib venv (due to the env var above) and pip-install these\n",
    "    RUNTIME_ENV[\"pip\"] = {\n",
    "        \"packages\": [\n",
    "            \"ray[default]==2.49.1\",\n",
    "            \"torch==2.3.1\",\n",
    "            \"torchvision==0.18.1\",\n",
    "            \"torchaudio==2.3.1\",\n",
    "        ],\n",
    "        # optional: pin pip itself to avoid old/broken versions on the node\n",
    "        \"pip_check\": True,\n",
    "    }\n",
    "else:\n",
    "    # Skip creating a new env; require packages preinstalled in the base image\n",
    "    RUNTIME_ENV[\"pip\"] = {\"packages\": [], \"preinstalled\": True}\n",
    "\n",
    "        \n",
    "# --------------------------------------\n",
    "if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "   addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "   ray.shutdown()\n",
    "   ray.init(\n",
    "      address=addr or \"auto\",\n",
    "      #runtime_env={\"env_vars\": ray_envs},   # same env you used earlier\n",
    "      namespace=\"demo-ray-ns\"\n",
    "  )\n",
    "# Connect to Domino Ray if available; otherw\n",
    "def main():\n",
    "    runtime_env = {        \n",
    "        \"env_vars\": RUN_CONFIG[\"env_vars\"],\n",
    "    }\n",
    "    env_keys = list(RUN_CONFIG[\"env_vars\"].keys())\n",
    "\n",
    "    # Launch two workers with the same runtime_env\n",
    "    t1 = probe_worker.options(runtime_env=runtime_env).remote(env_keys)\n",
    "    t2 = probe_worker.options(runtime_env=runtime_env).remote(env_keys)\n",
    "\n",
    "    out1, out2 = ray.get([t1, t2])\n",
    "    print(json.dumps({\"worker_1\": out1, \"worker_2\": out2}, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa8252e-696f-4b3d-90c9-35695188045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 22:27:44,419\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n",
      "2025-09-19 22:27:45,439\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"worker_1\": {\n",
      "    \"wrote\": \"/mnt/data/ddl-end-to-end-demo/12000000/96c5738fe28f186fffffffffffffffffffffffff12000000/probe_12000000_96c5738fe28f186fffffffffffffffffffffffff12000000_1266.json\",\n",
      "    \"size\": 17092\n",
      "  },\n",
      "  \"worker_2\": {\n",
      "    \"wrote\": \"/mnt/data/ddl-end-to-end-demo/12000000/a936e6d80473d7eeffffffffffffffffffffffff12000000/probe_12000000_a936e6d80473d7eeffffffffffffffffffffffff12000000_1265.json\",\n",
      "    \"size\": 17092\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# file: simple_ray_runtime_env_fix.py\n",
    "import os, json, platform\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "SHARED_DIR = \"/mnt/data/ddl-end-to-end-demo\"\n",
    "\n",
    "RUNTIME_ENV = {\n",
    "    \"env_vars\": {\n",
    "        \"SHARED_DIR\": SHARED_DIR,\n",
    "        \"APP_MODE\": \"probe\",\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "    }\n",
    "}\n",
    "\n",
    "if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "   addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "   ray.shutdown()\n",
    "   ray.init(\n",
    "      address=addr or \"auto\",\n",
    "      runtime_env=RUNTIME_ENV,\n",
    "      namespace=\"demo-ray-ns\"\n",
    "  )\n",
    "\n",
    "def _norm_id(val):\n",
    "    try:\n",
    "        return val.hex()\n",
    "    except Exception:\n",
    "        return str(val)\n",
    "\n",
    "@ray.remote(num_cpus=1)\n",
    "def probe_worker():\n",
    "    import os, time, socket, json, tempfile\n",
    "    from ray.runtime_context import get_runtime_context\n",
    "    from pathlib import Path\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "    job_id  = _norm_id(ctx.get_job_id())  if hasattr(ctx, \"get_job_id\") else \"na\"\n",
    "    task_id = _norm_id(ctx.get_task_id()) if hasattr(ctx, \"get_task_id\") else \"na\"\n",
    "\n",
    "    shared = Path(os.environ[\"SHARED_DIR\"],job_id,task_id)  # pulled from runtime_env\n",
    "    shared.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    payload = {\n",
    "        \"node\": platform.node(),\n",
    "        \"pid\": os.getpid(),\n",
    "        \"job_id\": job_id,\n",
    "        \"task_id\": task_id,\n",
    "        \"env\": dict(os.environ),\n",
    "        \"ts\": time.time(),\n",
    "    }\n",
    "\n",
    "    fname = f\"probe_{job_id}_{task_id}_{os.getpid()}.json\"\n",
    "    dest = shared / fname\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=str(shared)) as tmp:\n",
    "        json.dump(payload, tmp, indent=2)\n",
    "        tmp.flush()\n",
    "        os.fsync(tmp.fileno())\n",
    "        tmp_path = tmp.name\n",
    "    os.replace(tmp_path, dest)\n",
    "\n",
    "    return {\"wrote\": str(dest), \"size\": dest.stat().st_size}\n",
    "\n",
    "def main():\n",
    "    # --------------------------------------\n",
    "    if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "       addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "       ray.shutdown()\n",
    "       ray.init(\n",
    "          address=addr or \"auto\",\n",
    "          #runtime_env={\"env_vars\": RUNTIME_ENV},   # same env you used earlier\n",
    "          namespace=\"demo-ray-ns\"\n",
    "      )\n",
    "# Connect to Domino Ray if available; otherw\n",
    "    t1 = probe_worker.options(runtime_env=RUNTIME_ENV).remote()\n",
    "    t2 = probe_worker.options(runtime_env=RUNTIME_ENV).remote()\n",
    "    out1, out2 = ray.get([t1, t2])\n",
    "    print(json.dumps({\"worker_1\": out1, \"worker_2\": out2}, indent=2))\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954b996-3636-4d50-a313-b87f85142629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import ray\n",
    "from ray import air, train\n",
    "from ray.tune.logger import CSVLoggerCallback, JsonLoggerCallback\n",
    "from ray.train import Checkpoint\n",
    "from ray.train.torch import TorchTrainer, get_device, prepare_model, prepare_data_loader\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1) Pre-download MNIST once on the driver ----------\n",
    "def prepare_mnist(data_root: Path):\n",
    "    data_root.mkdir(parents=True, exist_ok=True)\n",
    "    # Uses torchvision's built-in downloader/extractor\n",
    "    from torchvision import datasets, transforms\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "    datasets.MNIST(str(data_root), train=True,  download=True, transform=tfm)\n",
    "    datasets.MNIST(str(data_root), train=False, download=True, transform=tfm)\n",
    "\n",
    "\n",
    "def build_model(num_classes: int = 10) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 512), nn.ReLU(),\n",
    "        nn.Linear(512, 256), nn.ReLU(),\n",
    "        nn.Linear(256, num_classes),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config: Dict):\n",
    "    device = get_device()\n",
    "    model = build_model().to(device)\n",
    "    model = prepare_model(model)\n",
    "\n",
    "    # Shared dataset root so rank-0 downloads once and all ranks read the same files\n",
    "    data_root = os.path.join(\n",
    "        os.environ.get(\"SHARED_DIR\"),\n",
    "        \"mnist\",\n",
    "    )\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    ''' Not avaiable in Ray 2.36\n",
    "    # Ensure only world rank 0 performs the download, others wait\n",
    "    with train.world_rank_zero_first():\n",
    "        datasets.MNIST(data_root, download=True, train=True, transform=transform)\n",
    "        datasets.MNIST(data_root, download=True, train=False, transform=transform)\n",
    "    '''\n",
    "    \n",
    "    train_ds = datasets.MNIST(data_root, train=True, transform=transform)\n",
    "    test_ds = datasets.MNIST(data_root, train=False, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    train_loader = prepare_data_loader(train_loader)\n",
    "    test_loader = prepare_data_loader(test_loader)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        # simple eval\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "                pred = model(x).argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.numel()\n",
    "        acc = correct / total\n",
    "\n",
    "        train.report({\"epoch\": epoch, \"train_loss\": running, \"val_acc\": acc})\n",
    "\n",
    "    # optional checkpoint (handles DDP-wrapped model)\n",
    "    state = model.module.state_dict() if hasattr(model, \"module\") else model.state_dict()\n",
    "    checkpoint = Checkpoint.from_dict({\"model_state\": state})\n",
    "    train.report({\"final_acc\": acc}, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995288e-e1aa-4b82-ad68-ad8246d9b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/mnt/data/ddl-end-to-end-demo/mnist/\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "prepare_mnist(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f2c80-3a59-4583-b7b4-3a76415b2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ray\n",
    "from ray.air.config import RunConfig\n",
    "from ray.tune.logger import CSVLoggerCallback, JsonLoggerCallback\n",
    "from ray.runtime_context import get_runtime_context\n",
    "from pathlib import Path\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    device = get_device()\n",
    "    model = prepare_model(build_model().to(device))\n",
    "\n",
    "    data_root = os.environ[\"SHARED_DIR\"]  # already populated\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # No network access in workers; just read the files\n",
    "    train_ds = datasets.MNIST(data_root, train=True,  download=False, transform=tfm)\n",
    "    test_ds  = datasets.MNIST(data_root, train=False, download=False, transform=tfm)\n",
    "\n",
    "    # Start conservative; you can raise num_workers/pin_memory after it works\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                              num_workers=0, pin_memory=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=512, shuffle=False,\n",
    "                              num_workers=0, pin_memory=False)\n",
    "\n",
    "    train_loader = prepare_data_loader(train_loader)\n",
    "    test_loader  = prepare_data_loader(test_loader)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = model(x).argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.numel()\n",
    "        acc = correct / total\n",
    "        train.report({\"epoch\": epoch, \"train_loss\": running, \"val_acc\": acc})\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    RUNTIME_ENV = {\n",
    "        \"env_vars\": {\n",
    "        \"GLOO_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"SHARED_DIR\": str(data_dir),\n",
    "        \"TUNE_DISABLE_AUTO_CALLBACKS\": \"1\",\n",
    "        \"TORCH_DISABLE_ADDR2LINE\": \"1\",     # stop symbolizer hang\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"NCCL_P2P_DISABLE\": \"1\",\n",
    "        \"NCCL_SHM_DISABLE\": \"1\",\n",
    "        \"OMP_NUM_THREADS\": \"2\",\n",
    "        # >>> Key bits for DDP rendezvous <<<\n",
    "        \"MASTER_PORT\": \"29400\",           # fixed, not ephemeral\n",
    "        \"GLOO_SOCKET_IFNAME\": \"eth0\",     # bind on pod interface\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",     # harmless even if CPU-only\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    # --------------------------------------\n",
    "    if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "       addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "       ray.shutdown()\n",
    "       ray.init(\n",
    "          address=addr or \"auto\",\n",
    "          runtime_env=RUNTIME_ENV,   # same env you used earlier\n",
    "          namespace=\"demo-ray-ns\"\n",
    "      )\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "    try:\n",
    "        job_id_hex = ctx.get_job_id().hex()\n",
    "    except Exception:\n",
    "        job_id_hex = \"unknown_job\"\n",
    "\n",
    "    DATASET_FOLDER = \"/mnt/data/ddl-end-to-end-demo/\"\n",
    "    shared = Path(DATASET_FOLDER,job_id_hex,\"ray_results\")  # pulled from runtime_env\n",
    "    shared.mkdir(parents=True, exist_ok=True)\n",
    "    STORAGE_PATH=str(shared)\n",
    "    \n",
    "    storage_base = Path(\"/mnt/data/ddl-end-to-end-demo\")  # head-visible shared\n",
    "    job_id_hex = getattr(ray.get_runtime_context(), \"get_job_id\", lambda: \"unknown\")()\n",
    "    job_id_hex = job_id_hex.hex() if hasattr(job_id_hex, \"hex\") else str(job_id_hex)\n",
    "    storage_path = str(storage_base / job_id_hex / \"ray_results\")\n",
    "    \n",
    "    os.environ[\"TUNE_DISABLE_AUTO_CALLBACKS\"] = \"1\"\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker,\n",
    "        train_loop_config={\"lr\": 1e-3, \"batch_size\": 256, \"epochs\": 5},\n",
    "        scaling_config=ScalingConfig(\n",
    "            num_workers=1,\n",
    "            use_gpu=False,                      # keep CPU+gloo until stable\n",
    "            resources_per_worker={\"CPU\": 2},\n",
    "            placement_strategy=\"PACK\",          # single-node to avoid networking issues\n",
    "        ),\n",
    "        run_config=RunConfig(\n",
    "            name=f\"mnist_torch_ddp_{job_id_hex}\",\n",
    "            storage_path=STORAGE_PATH,\n",
    "            callbacks=[CSVLoggerCallback(), JsonLoggerCallback()],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff962e3-ae37-4083-8517-dd95b76b9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea18d3-8956-48b7-84fe-5efab74508dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45367d66-4f56-4e1a-98b0-44504bd70657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
