{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a54bc1-58ed-42ae-ae0c-341a0ca2d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1336f048-16eb-441b-9c12-74309c613578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import ray\n",
    "from ray import air, train\n",
    "from ray.train import Checkpoint\n",
    "from ray.train.torch import TorchTrainer, get_device, prepare_model, prepare_data_loader\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "\n",
    "try:\n",
    "    from ray.tune.callback import Callback      # Ray >= 2.6\n",
    "except ImportError:\n",
    "    from ray.tune.callbacks import Callback     # Older Ray\n",
    "from utils import ddl_cluster_scaling_client\n",
    "from utils import mlflow_utils\n",
    "from utils import ray_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe473df-6ff9-4cce-8bbf-10f4493cb6dd",
   "metadata": {},
   "source": [
    "## Pre-requsites\n",
    "\n",
    "Configure the following user environment variables\n",
    "\n",
    "1. AWS_ROLE_ARN - This is the AWS role being assumed via IR\n",
    "2. S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8af9bda-73fe-4dde-b2b5-c9c6e54ee9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../tmp/train.parquet to s3://ddl-wadkars/end-to-end/california/train/train.parquet\n",
      "upload: ../../../tmp/val.parquet to s3://ddl-wadkars/end-to-end/california/val/val.parquet\n",
      "upload: ../../../tmp/test.parquet to s3://ddl-wadkars/end-to-end/california/test/test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download dataset and push to S3\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.rename(columns={\"MedHouseVal\": \"median_house_value\"})\n",
    "\n",
    "# Split\n",
    "train, tmp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val, test  = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save locally\n",
    "train.to_parquet(\"/tmp/train.parquet\", index=False)\n",
    "val.to_parquet(\"/tmp/val.parquet\", index=False)\n",
    "test.to_parquet(\"/tmp/test.parquet\", index=False)\n",
    "\n",
    "# Push to S3\n",
    "!aws s3 cp /tmp/train.parquet s3://${S3_BUCKET_NAME}/end-to-end/california/train/\n",
    "!aws s3 cp /tmp/val.parquet   s3://${S3_BUCKET_NAME}/end-to-end/california/val/\n",
    "!aws s3 cp /tmp/test.parquet  s3://${S3_BUCKET_NAME}/end-to-end/california/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7040bd1-cd6e-4be0-98d9-2f90d4a527ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kind = \"rayclusters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ca4926-350b-4256-9377-6fda5f7e8d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Status code 200\n",
      "ray-68cdcd8314e2171c9b0f2508\n",
      "http://ddl-cluster-scaler-svc.domino-field.svc.cluster.local/ddl_cluster_scaler/cluster/rayclusters/ray-68cdcd8314e2171c9b0f2508\n",
      "Status code 200\n",
      "Expected worker nodes 2\n",
      "Current worker nodes ['ray-68cdcd8314e2171c9b0f2508-ray-worker-0']\n",
      "Scaling not yet done...\n",
      "ray-68cdcd8314e2171c9b0f2508\n",
      "http://ddl-cluster-scaler-svc.domino-field.svc.cluster.local/ddl_cluster_scaler/cluster/rayclusters/ray-68cdcd8314e2171c9b0f2508\n",
      "Status code 200\n",
      "Expected worker nodes 2\n",
      "Current worker nodes ['ray-68cdcd8314e2171c9b0f2508-ray-worker-0', 'ray-68cdcd8314e2171c9b0f2508-ray-worker-1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = ddl_cluster_scaling_client.scale_cluster(cluster_kind=cluster_kind,head_hw_tier_name=\"Medium\", worker_hw_tier_name=\"Medium\", replicas=2)\n",
    "json.dumps(j, indent=2, sort_keys=True, ensure_ascii=False)\n",
    "ddl_cluster_scaling_client.wait_until_scaling_complete(cluster_kind=cluster_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55bb35-622c-48cc-8204-fc219791c577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ab02e5-1bff-4223-a424-8a1a78496bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code 202\n",
      "2025-09-19T21:45:53Z\n",
      "Status code 200\n",
      "Status code 200\n",
      "not_restarted\n",
      "Wait 3 seconds before checking again\n",
      "Status code 200\n",
      "not_restarted\n",
      "Wait 3 seconds before checking again\n",
      "Status code 200\n",
      "not_restarted\n",
      "Wait 3 seconds before checking again\n",
      "Status code 200\n",
      "restarted_but_not_ready\n",
      "Wait 3 seconds before checking again\n",
      "Status code 200\n",
      "restarted_but_not_ready\n",
      "Wait 3 seconds before checking again\n",
      "Status code 200\n",
      "restarted_and_ready\n"
     ]
    }
   ],
   "source": [
    "from utils import ddl_cluster_scaling_client\n",
    "j = ddl_cluster_scaling_client.restart_head_node(cluster_kind=\"rayclusters\")\n",
    "restarts_at = j['started_at']\n",
    "print(restarts_at)\n",
    "ddl_cluster_scaling_client.restart_head_node_status(cluster_kind=\"rayclusters\",restarted_since=restarts_at)\n",
    "ddl_cluster_scaling_client.wait_until_node_restarted(cluster_kind=\"rayclusters\",restarted_since=restarts_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c7b117-675f-450b-8076-fcef8bf7771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import json\n",
    "import ray\n",
    "\n",
    "# ----- RUN CONFIG (edit as needed) -----\n",
    "\n",
    "'''\n",
    "#Pip installs do not work yet in Domino. Add the libraries to your environment or install using command line in the worker\n",
    "RUN_CONFIG = {\n",
    "    # Packages are installed into an isolated env for each worker by Ray\n",
    "    \n",
    "    \"pip\": [\n",
    "        \"ray[train]==2.49.1\",\n",
    "        \"torch==2.3.1\",\n",
    "        \"torchvision==0.18.1\",\n",
    "        \"torchaudio==2.3.1\"\n",
    "    ],\n",
    "    # Environment variables to expose inside workers\n",
    "    \"env_vars\": {\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",     # harmless on CPU-only boxes; useful hint on GPU clusters without IB/EFA\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\"\n",
    "    }\n",
    "}\n",
    "'''\n",
    "RUN_CONFIG = {\n",
    "    # Packages are installed into an isolated env for each worker by Ray\n",
    "    # Environment variables to expose inside workers\n",
    "    \"env_vars\": {\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",     # harmless on CPU-only boxes; useful hint on GPU clusters without IB/EFA\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae4e00d6-0f97-4180-a994-7cb7fc0e83ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.1 in /opt/conda/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision==0.18.1 in /opt/conda/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio==2.3.1 in /opt/conda/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.1) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.1) (10.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ce6fe0-a01c-410d-9b65-fd2843dcf049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyopenssl<24\n",
      "  Downloading pyOpenSSL-23.3.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting cryptography<42\n",
      "  Downloading cryptography-41.0.7-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography<42) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography<42) (2.22)\n",
      "Downloading pyOpenSSL-23.3.0-py3-none-any.whl (58 kB)\n",
      "Downloading cryptography-41.0.7-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cryptography, pyopenssl\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 42.0.5\n",
      "    Uninstalling cryptography-42.0.5:\n",
      "      Successfully uninstalled cryptography-42.0.5\n",
      "  Attempting uninstall: pyopenssl\n",
      "    Found existing installation: pyOpenSSL 24.2.1\n",
      "    Uninstalling pyOpenSSL-24.2.1:\n",
      "      Successfully uninstalled pyOpenSSL-24.2.1\n",
      "Successfully installed cryptography-41.0.7 pyopenssl-23.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"pyopenssl<24\" \"cryptography<42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a23211-b6aa-469e-878f-b19ec0943134",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "def probe_worker(env_keys):\n",
    "    import os, platform, importlib\n",
    "    from ray.runtime_context import get_runtime_context\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "\n",
    "    def _id(ctx, name):\n",
    "        if not hasattr(ctx, name):\n",
    "            return None\n",
    "        v = getattr(ctx, name)()  # may be bytes-like w/ .hex() or already a str\n",
    "        try:\n",
    "            return v.hex()\n",
    "        except AttributeError:\n",
    "            return str(v)\n",
    "\n",
    "    # Torch proof without leaking torch objects in the return\n",
    "    torch = importlib.import_module(\"torch\")\n",
    "    x = torch.tensor([1.0, 2.0, 3.0]) * 2.0\n",
    "    sample_sum = float(x.sum().item())\n",
    "    torch_version = str(getattr(torch, \"__version__\", \"unknown\"))\n",
    "    cuda_avail = bool(hasattr(torch, \"cuda\") and torch.cuda.is_available())\n",
    "    del x, torch\n",
    "\n",
    "    return {\n",
    "        \"node\": platform.node(),\n",
    "        \"pid\": os.getpid(),\n",
    "        \"python_executable\": os.sys.executable,\n",
    "        \"torch_version\": torch_version,\n",
    "        \"torch_cuda_available\": cuda_avail,\n",
    "        \"env\": {k: os.environ.get(k) for k in env_keys},\n",
    "        \"torch_sample_sum\": sample_sum,\n",
    "        \"ids\": {\n",
    "            \"task_id\": _id(ctx, \"get_task_id\"),\n",
    "            \"actor_id\": _id(ctx, \"get_actor_id\"),\n",
    "            \"node_id\": _id(ctx, \"get_node_id\"),\n",
    "            \"job_id\": _id(ctx, \"get_job_id\"),\n",
    "            \"namespace\": ctx.get_namespace() if hasattr(ctx, \"get_namespace\") else None,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10dd3a8b-6669-45fa-9368-dc722d74746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 21:46:23,529\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n"
     ]
    }
   ],
   "source": [
    "# file: simple_ray_runtime_env_fix.py\n",
    "import os, json, platform\n",
    "import ray\n",
    "import torch\n",
    "# ---- hard fix for your error: avoid virtualenv, use stdlib venv ----\n",
    "os.environ[\"RAY_RUNTIME_ENV_VENV_CREATION\"] = \"venv\"  # must be set before ray.init\n",
    "\n",
    "# ---- choose one path ----\n",
    "USE_PREINSTALLED = False  # True = skip installs; all deps must already be on nodes\n",
    "\n",
    "RUNTIME_ENV = {\n",
    "    \"env_vars\": {\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "    },\n",
    "    \"eager_install\": True,  # fail fast during setup\n",
    "}\n",
    "\n",
    "if not USE_PREINSTALLED:\n",
    "    # Ray will create a venv using stdlib venv (due to the env var above) and pip-install these\n",
    "    RUNTIME_ENV[\"pip\"] = {\n",
    "        \"packages\": [\n",
    "            \"ray[default]==2.49.1\",\n",
    "            \"torch==2.3.1\",\n",
    "            \"torchvision==0.18.1\",\n",
    "            \"torchaudio==2.3.1\",\n",
    "        ],\n",
    "        # optional: pin pip itself to avoid old/broken versions on the node\n",
    "        \"pip_check\": True,\n",
    "    }\n",
    "else:\n",
    "    # Skip creating a new env; require packages preinstalled in the base image\n",
    "    RUNTIME_ENV[\"pip\"] = {\"packages\": [], \"preinstalled\": True}\n",
    "\n",
    "        \n",
    "# --------------------------------------\n",
    "if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "   addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "   ray.shutdown()\n",
    "   ray.init(\n",
    "      address=addr or \"auto\",\n",
    "      #runtime_env={\"env_vars\": ray_envs},   # same env you used earlier\n",
    "      namespace=\"demo-ray-ns\"\n",
    "  )\n",
    "# Connect to Domino Ray if available; otherw\n",
    "def main():\n",
    "    runtime_env = {        \n",
    "        \"env_vars\": RUN_CONFIG[\"env_vars\"],\n",
    "    }\n",
    "    env_keys = list(RUN_CONFIG[\"env_vars\"].keys())\n",
    "\n",
    "    # Launch two workers with the same runtime_env\n",
    "    t1 = probe_worker.options(runtime_env=runtime_env).remote(env_keys)\n",
    "    t2 = probe_worker.options(runtime_env=runtime_env).remote(env_keys)\n",
    "\n",
    "    out1, out2 = ray.get([t1, t2])\n",
    "    print(json.dumps({\"worker_1\": out1, \"worker_2\": out2}, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baa8252e-696f-4b3d-90c9-35695188045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 21:46:24,894\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n",
      "2025-09-19 21:46:25,911\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"worker_1\": {\n",
      "    \"wrote\": \"/mnt/data/ddl-end-to-end-demo/03000000/fa31b89f94899135ffffffffffffffffffffffff03000000/probe_03000000_fa31b89f94899135ffffffffffffffffffffffff03000000_150.json\",\n",
      "    \"size\": 17091\n",
      "  },\n",
      "  \"worker_2\": {\n",
      "    \"wrote\": \"/mnt/data/ddl-end-to-end-demo/03000000/44bfa4a5859b21acffffffffffffffffffffffff03000000/probe_03000000_44bfa4a5859b21acffffffffffffffffffffffff03000000_151.json\",\n",
      "    \"size\": 17091\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# file: simple_ray_runtime_env_fix.py\n",
    "import os, json, platform\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "SHARED_DIR = \"/mnt/data/ddl-end-to-end-demo\"\n",
    "\n",
    "RUNTIME_ENV = {\n",
    "    \"env_vars\": {\n",
    "        \"SHARED_DIR\": SHARED_DIR,\n",
    "        \"APP_MODE\": \"probe\",\n",
    "        \"MY_APP_FLAG\": \"enabled\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "    }\n",
    "}\n",
    "\n",
    "if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "   addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "   ray.shutdown()\n",
    "   ray.init(\n",
    "      address=addr or \"auto\",\n",
    "      runtime_env=RUNTIME_ENV,\n",
    "      namespace=\"demo-ray-ns\"\n",
    "  )\n",
    "\n",
    "def _norm_id(val):\n",
    "    try:\n",
    "        return val.hex()\n",
    "    except Exception:\n",
    "        return str(val)\n",
    "\n",
    "@ray.remote(num_cpus=1)\n",
    "def probe_worker():\n",
    "    import os, time, socket, json, tempfile\n",
    "    from ray.runtime_context import get_runtime_context\n",
    "    from pathlib import Path\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "    job_id  = _norm_id(ctx.get_job_id())  if hasattr(ctx, \"get_job_id\") else \"na\"\n",
    "    task_id = _norm_id(ctx.get_task_id()) if hasattr(ctx, \"get_task_id\") else \"na\"\n",
    "\n",
    "    shared = Path(os.environ[\"SHARED_DIR\"],job_id,task_id)  # pulled from runtime_env\n",
    "    shared.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    payload = {\n",
    "        \"node\": platform.node(),\n",
    "        \"pid\": os.getpid(),\n",
    "        \"job_id\": job_id,\n",
    "        \"task_id\": task_id,\n",
    "        \"env\": dict(os.environ),\n",
    "        \"ts\": time.time(),\n",
    "    }\n",
    "\n",
    "    fname = f\"probe_{job_id}_{task_id}_{os.getpid()}.json\"\n",
    "    dest = shared / fname\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=str(shared)) as tmp:\n",
    "        json.dump(payload, tmp, indent=2)\n",
    "        tmp.flush()\n",
    "        os.fsync(tmp.fileno())\n",
    "        tmp_path = tmp.name\n",
    "    os.replace(tmp_path, dest)\n",
    "\n",
    "    return {\"wrote\": str(dest), \"size\": dest.stat().st_size}\n",
    "\n",
    "def main():\n",
    "    # --------------------------------------\n",
    "    if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "       addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "       ray.shutdown()\n",
    "       ray.init(\n",
    "          address=addr or \"auto\",\n",
    "          #runtime_env={\"env_vars\": RUNTIME_ENV},   # same env you used earlier\n",
    "          namespace=\"demo-ray-ns\"\n",
    "      )\n",
    "# Connect to Domino Ray if available; otherw\n",
    "    t1 = probe_worker.options(runtime_env=RUNTIME_ENV).remote()\n",
    "    t2 = probe_worker.options(runtime_env=RUNTIME_ENV).remote()\n",
    "    out1, out2 = ray.get([t1, t2])\n",
    "    print(json.dumps({\"worker_1\": out1, \"worker_2\": out2}, indent=2))\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d954b996-3636-4d50-a313-b87f85142629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import ray\n",
    "from ray import air, train\n",
    "from ray.tune.logger import CSVLoggerCallback, JsonLoggerCallback\n",
    "from ray.train import Checkpoint\n",
    "from ray.train.torch import TorchTrainer, get_device, prepare_model, prepare_data_loader\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1) Pre-download MNIST once on the driver ----------\n",
    "def prepare_mnist(data_root: Path):\n",
    "    data_root.mkdir(parents=True, exist_ok=True)\n",
    "    # Uses torchvision's built-in downloader/extractor\n",
    "    from torchvision import datasets, transforms\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "    datasets.MNIST(str(data_root), train=True,  download=True, transform=tfm)\n",
    "    datasets.MNIST(str(data_root), train=False, download=True, transform=tfm)\n",
    "\n",
    "\n",
    "def build_model(num_classes: int = 10) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 512), nn.ReLU(),\n",
    "        nn.Linear(512, 256), nn.ReLU(),\n",
    "        nn.Linear(256, num_classes),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config: Dict):\n",
    "    device = get_device()\n",
    "    model = build_model().to(device)\n",
    "    model = prepare_model(model)\n",
    "\n",
    "    # Shared dataset root so rank-0 downloads once and all ranks read the same files\n",
    "    data_root = os.path.join(\n",
    "        os.environ.get(\"SHARED_DIR\"),\n",
    "        \"mnist\",\n",
    "    )\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    ''' Not avaiable in Ray 2.36\n",
    "    # Ensure only world rank 0 performs the download, others wait\n",
    "    with train.world_rank_zero_first():\n",
    "        datasets.MNIST(data_root, download=True, train=True, transform=transform)\n",
    "        datasets.MNIST(data_root, download=True, train=False, transform=transform)\n",
    "    '''\n",
    "    \n",
    "    train_ds = datasets.MNIST(data_root, train=True, transform=transform)\n",
    "    test_ds = datasets.MNIST(data_root, train=False, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    train_loader = prepare_data_loader(train_loader)\n",
    "    test_loader = prepare_data_loader(test_loader)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        # simple eval\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "                pred = model(x).argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.numel()\n",
    "        acc = correct / total\n",
    "\n",
    "        train.report({\"epoch\": epoch, \"train_loss\": running, \"val_acc\": acc})\n",
    "\n",
    "    # optional checkpoint (handles DDP-wrapped model)\n",
    "    state = model.module.state_dict() if hasattr(model, \"module\") else model.state_dict()\n",
    "    checkpoint = Checkpoint.from_dict({\"model_state\": state})\n",
    "    train.report({\"final_acc\": acc}, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d995288e-e1aa-4b82-ad68-ad8246d9b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/mnt/data/ddl-end-to-end-demo/mnist/\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "prepare_mnist(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d34f2c80-3a59-4583-b7b4-3a76415b2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ray\n",
    "from ray.air.config import RunConfig\n",
    "from ray.tune.logger import CSVLoggerCallback, JsonLoggerCallback\n",
    "from ray.runtime_context import get_runtime_context\n",
    "from pathlib import Path\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    device = get_device()\n",
    "    model = prepare_model(build_model().to(device))\n",
    "\n",
    "    data_root = os.environ[\"SHARED_DIR\"]  # already populated\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # No network access in workers; just read the files\n",
    "    train_ds = datasets.MNIST(data_root, train=True,  download=False, transform=tfm)\n",
    "    test_ds  = datasets.MNIST(data_root, train=False, download=False, transform=tfm)\n",
    "\n",
    "    # Start conservative; you can raise num_workers/pin_memory after it works\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                              num_workers=0, pin_memory=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=512, shuffle=False,\n",
    "                              num_workers=0, pin_memory=False)\n",
    "\n",
    "    train_loader = prepare_data_loader(train_loader)\n",
    "    test_loader  = prepare_data_loader(test_loader)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = model(x).argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.numel()\n",
    "        acc = correct / total\n",
    "        train.report({\"epoch\": epoch, \"train_loss\": running, \"val_acc\": acc})\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    RUNTIME_ENV = {\n",
    "        \"env_vars\": {\n",
    "        \"GLOO_SOCKET_IFNAME\": \"eth0\",\n",
    "        \"SHARED_DIR\": str(data_dir),\n",
    "        \"TUNE_DISABLE_AUTO_CALLBACKS\": \"1\",\n",
    "        \"TORCH_DISABLE_ADDR2LINE\": \"1\",     # stop symbolizer hang\n",
    "        \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "        \"NCCL_IB_DISABLE\": \"1\",\n",
    "        \"NCCL_P2P_DISABLE\": \"1\",\n",
    "        \"NCCL_SHM_DISABLE\": \"1\",\n",
    "        \"OMP_NUM_THREADS\": \"2\",\n",
    "        # >>> Key bits for DDP rendezvous <<<\n",
    "        \"MASTER_PORT\": \"29400\",           # fixed, not ephemeral\n",
    "        \"GLOO_SOCKET_IFNAME\": \"eth0\",     # bind on pod interface\n",
    "        \"NCCL_SOCKET_IFNAME\": \"eth0\",     # harmless even if CPU-only\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    # --------------------------------------\n",
    "    if \"RAY_HEAD_SERVICE_HOST\" in os.environ and \"RAY_HEAD_SERVICE_PORT\" in os.environ:\n",
    "       addr = f\"ray://{os.environ['RAY_HEAD_SERVICE_HOST']}:{os.environ['RAY_HEAD_SERVICE_PORT']}\"\n",
    "       ray.shutdown()\n",
    "       ray.init(\n",
    "          address=addr or \"auto\",\n",
    "          runtime_env=RUNTIME_ENV,   # same env you used earlier\n",
    "          namespace=\"demo-ray-ns\"\n",
    "      )\n",
    "\n",
    "    ctx = get_runtime_context()\n",
    "    try:\n",
    "        job_id_hex = ctx.get_job_id().hex()\n",
    "    except Exception:\n",
    "        job_id_hex = \"unknown_job\"\n",
    "\n",
    "    DATASET_FOLDER = \"/mnt/data/ddl-end-to-end-demo/\"\n",
    "    shared = Path(DATASET_FOLDER,job_id_hex,\"ray_results\")  # pulled from runtime_env\n",
    "    shared.mkdir(parents=True, exist_ok=True)\n",
    "    STORAGE_PATH=str(shared)\n",
    "    \n",
    "    storage_base = Path(\"/mnt/data/ddl-end-to-end-demo\")  # head-visible shared\n",
    "    job_id_hex = getattr(ray.get_runtime_context(), \"get_job_id\", lambda: \"unknown\")()\n",
    "    job_id_hex = job_id_hex.hex() if hasattr(job_id_hex, \"hex\") else str(job_id_hex)\n",
    "    storage_path = str(storage_base / job_id_hex / \"ray_results\")\n",
    "    \n",
    "    os.environ[\"TUNE_DISABLE_AUTO_CALLBACKS\"] = \"1\"\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker,\n",
    "        train_loop_config={\"lr\": 1e-3, \"batch_size\": 256, \"epochs\": 5},\n",
    "        scaling_config=ScalingConfig(\n",
    "            num_workers=2,\n",
    "            use_gpu=False,                      # keep CPU+gloo until stable\n",
    "            resources_per_worker={\"CPU\": 2},\n",
    "            placement_strategy=\"PACK\",          # single-node to avoid networking issues\n",
    "        ),\n",
    "        run_config=RunConfig(\n",
    "            name=f\"mnist_torch_ddp_{job_id_hex}\",\n",
    "            storage_path=STORAGE_PATH,\n",
    "            callbacks=[CSVLoggerCallback(), JsonLoggerCallback()],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eff962e3-ae37-4083-8517-dd95b76b9420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m View detailed results here: /mnt/data/ddl-end-to-end-demo/unknown_job/ray_results/mnist_torch_ddp_0e000000\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-09-19_14-46-06_080045_1/artifacts/2025-09-19_15-23-25/mnist_torch_ddp_0e000000/driver_artifacts`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m Training started with configuration:\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m ╭──────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m │ Training config                      │\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m ├──────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m │ train_loop_config/batch_size     256 │\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m │ train_loop_config/epochs           5 │\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m │ train_loop_config/lr           0.001 │\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m ╰──────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m Training errored after 0 iterations at 2025-09-19 15:23:31. Total running time: 5s\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m Error file: /tmp/ray/session_2025-09-19_14-46-06_080045_1/artifacts/2025-09-19_15-23-25/mnist_torch_ddp_0e000000/driver_artifacts/TorchTrainer_42653_00000_0_2025-09-19_15-23-25/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m Trial task failed for trial TorchTrainer_42653_00000\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py\", line 2664, in get\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m ray.exceptions.RayTaskError(DistNetworkError): \u001b[36mray::TrainTrainable.train()\u001b[39m (pid=1046, ip=100.64.35.141, actor_id=ae1cf513ec6d32f4e43d334b0e000000, repr=TorchTrainer)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 799, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 107, in _train_coordinator_fn\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     trainer.training_loop()\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 459, in training_loop\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     backend_executor.start()\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 203, in start\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     self._backend.on_start(self.worker_group, self._backend_config)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/torch/config.py\", line 200, in on_start\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     ray.get(setup_futures)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m ray.exceptions.RayTaskError(DistNetworkError): \u001b[36mray::_RayTrainWorker__execute._setup_torch_process_group()\u001b[39m (pid=456, ip=100.64.71.249, actor_id=59667c756cd57fb591cdc7bf0e000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f50b0fe6680>)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 30, in __execute\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/torch/config.py\", line 115, in _setup_torch_process_group\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     dist.init_process_group(\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 89, in wrapper\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     func_return = func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 1305, in init_process_group\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     store, rank, world_size = next(rendezvous_iterator)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 246, in _env_rendezvous_handler\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 174, in _create_c10d_store\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m     return TCPStore(\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m torch.distributed.DistNetworkError: Connection reset by peer\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m Exception raised from recvBytes at ../torch/csrc/distributed/c10d/Utils.hpp:669 (most recent call first):\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f50b00a9897 in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #1: <unknown function> + 0x5b39ecc (0x7f508a1bfecc in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #2: c10d::TCPStore::incrementValueBy(std::string const&, long) + 0x155 (0x7f508a1bae65 in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #3: c10d::TCPStore::waitForWorkers() + 0xbb (0x7f508a1bdb6b in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #4: c10d::TCPStore::TCPStore(std::string, c10d::TCPStoreOptions const&) + 0x64c (0x7f508a1be3ec in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #5: <unknown function> + 0xce1aad (0x7f50aa3dcaad in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #6: <unknown function> + 0x47def4 (0x7f50a9b78ef4 in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #7: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x4fdc87]\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #8: _PyObject_MakeTpCall + 0x25b (0x4f741b in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #9: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x509cbf]\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #10: PyVectorcall_Call + 0xb9 (0x50a869 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #11: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x507a1c]\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #12: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x4f7786]\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #13: <unknown function> + 0x47c74b (0x7f50a9b7774b in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #14: _PyObject_MakeTpCall + 0x25b (0x4f741b in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #15: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #16: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #17: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #18: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x571c97]\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #19: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x4fe2b4]\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #20: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #21: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #22: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #23: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #25: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #26: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #27: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #28: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #29: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #30: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #33: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #34: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #35: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #36: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #39: PyVectorcall_Call + 0xb9 (0x50a869 in ray::_RayTrainWorker__execute._setup_torch_process_group)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #40: <unknown function> + 0x64b9cf (0x7f525d5459cf in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #41: <unknown function> + 0x6a2621 (0x7f525d59c621 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #42: <unknown function> + 0x64b9cf (0x7f525d5459cf in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #43: <unknown function> + 0x734c16 (0x7f525d62ec16 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #44: std::_Function_handler<ray::Status (ray::rpc::Address const&, ray::rpc::TaskType, std::string, ray::core::RayFunction const&, std::unordered_map<std::string, double, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, double> > > const&, std::vector<std::shared_ptr<ray::RayObject>, std::allocator<std::shared_ptr<ray::RayObject> > > const&, std::vector<ray::rpc::ObjectReference, std::allocator<ray::rpc::ObjectReference> > const&, std::string const&, std::string const&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, std::shared_ptr<ray::LocalMemoryBuffer>&, bool*, std::string*, std::vector<ray::ConcurrencyGroup, std::allocator<ray::ConcurrencyGroup> > const&, std::string, bool, bool, bool, long), ray::Status (*)(ray::rpc::Address const&, ray::rpc::TaskType, std::string, ray::core::RayFunction const&, std::unordered_map<std::string, double, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, double> > > const&, std::vector<std::shared_ptr<ray::RayObject>, std::allocator<std::shared_ptr<ray::RayObject> > > const&, std::vector<ray::rpc::ObjectReference, std::allocator<ray::rpc::ObjectReference> > const&, std::string, std::string, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, std::shared_ptr<ray::LocalMemoryBuffer>&, bool*, std::string*, std::vector<ray::ConcurrencyGroup, std::allocator<ray::ConcurrencyGroup> > const&, std::string, bool, bool, bool, long)>::_M_invoke(std::_Any_data const&, ray::rpc::Address const&, ray::rpc::TaskType&&, std::string&&, ray::core::RayFunction const&, std::unordered_map<std::string, double, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, double> > > const&, std::vector<std::shared_ptr<ray::RayObject>, std::allocator<std::shared_ptr<ray::RayObject> > > const&, std::vector<ray::rpc::ObjectReference, std::allocator<ray::rpc::ObjectReference> > const&, std::string const&, std::string const&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*&&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*&&, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*&&, std::shared_ptr<ray::LocalMemoryBuffer>&, bool*&&, std::string*&&, std::vector<ray::ConcurrencyGroup, std::allocator<ray::ConcurrencyGroup> > const&, std::string&&, bool&&, bool&&, bool&&, long&&) + 0x169 (0x7f525d54baa9 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #45: ray::core::CoreWorker::ExecuteTask(ray::TaskSpecification const&, std::shared_ptr<std::unordered_map<std::string, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > > > > > > const&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, google::protobuf::RepeatedPtrField<ray::rpc::ObjectReferenceCount>*, bool*, std::string*) + 0xd50 (0x7f525d763610 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #46: std::_Function_handler<ray::Status (ray::TaskSpecification const&, std::shared_ptr<std::unordered_map<std::string, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > > > > > >, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, google::protobuf::RepeatedPtrField<ray::rpc::ObjectReferenceCount>*, bool*, std::string*), std::_Bind<ray::Status (ray::core::CoreWorker::*(ray::core::CoreWorker*, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>, std::_Placeholder<6>, std::_Placeholder<7>, std::_Placeholder<8>))(ray::TaskSpecification const&, std::shared_ptr<std::unordered_map<std::string, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > > > > > > const&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, google::protobuf::RepeatedPtrField<ray::rpc::ObjectReferenceCount>*, bool*, std::string*)> >::_M_invoke(std::_Any_data const&, ray::TaskSpecification const&, std::shared_ptr<std::unordered_map<std::string, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > > > > > >&&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*&&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*&&, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*&&, google::protobuf::RepeatedPtrField<ray::rpc::ObjectReferenceCount>*&&, bool*&&, std::string*&&) + 0x58 (0x7f525d68db98 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #47: <unknown function> + 0x8718c4 (0x7f525d76b8c4 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #48: <unknown function> + 0x872cba (0x7f525d76ccba in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #49: <unknown function> + 0x8ab91e (0x7f525d7a591e in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #50: ray::core::ActorSchedulingQueue::AcceptRequestOrRejectIfCanceled(ray::TaskID, ray::core::InboundRequest&) + 0x114 (0x7f525d7a6994 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #51: <unknown function> + 0x8b0673 (0x7f525d7aa673 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #52: ray::core::ActorSchedulingQueue::Add(long, long, std::function<void (std::function<void (ray::Status, std::function<void ()>, std::function<void ()>)>)>, std::function<void (ray::Status const&, std::function<void (ray::Status, std::function<void ()>, std::function<void ()>)>)>, std::function<void (ray::Status, std::function<void ()>, std::function<void ()>)>, std::string const&, std::shared_ptr<ray::FunctionDescriptorInterface> const&, ray::TaskID, std::vector<ray::rpc::ObjectReference, std::allocator<ray::rpc::ObjectReference> > const&) + 0x4b3 (0x7f525d7a9083 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #53: ray::core::TaskReceiver::HandleTask(ray::rpc::PushTaskRequest const&, ray::rpc::PushTaskReply*, std::function<void (ray::Status, std::function<void ()>, std::function<void ()>)>) + 0x1137 (0x7f525d76e347 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #54: <unknown function> + 0x7b097d (0x7f525d6aa97d in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #55: <unknown function> + 0xb3ea1c (0x7f525da38a1c in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #56: <unknown function> + 0xb3a5fe (0x7f525da345fe in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #57: <unknown function> + 0xb3aa76 (0x7f525da34a76 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #58: <unknown function> + 0x110c59b (0x7f525e00659b in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #59: <unknown function> + 0x110df19 (0x7f525e007f19 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #60: <unknown function> + 0x110e622 (0x7f525e008622 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #61: ray::core::CoreWorker::RunTaskExecutionLoop() + 0xcd (0x7f525d6a6a8d in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #62: ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop() + 0x85 (0x7f525d770315 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m frame #63: ray::core::CoreWorkerProcess::RunTaskExecutionLoop() + 0x1d (0x7f525d77051d in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\n",
      "\u001b[36m(RayTrainWorker pid=1091, ip=100.64.35.141)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/mnt/data/ddl-end-to-end-demo/unknown_job/ray_results/mnist_torch_ddp_0e000000\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(DistNetworkError)\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;31mRayTaskError(DistNetworkError)\u001b[0m: \u001b[36mray::TrainTrainable.train()\u001b[39m (pid=1046, ip=100.64.35.141, actor_id=ae1cf513ec6d32f4e43d334b0e000000, repr=TorchTrainer)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n    raise skipped from exception_cause(skipped)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 98, in run\n    self._ret = self._target(*self._args, **self._kwargs)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n    training_func=lambda: self._trainable_func(self.config),\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 799, in _trainable_func\n  File \"/opt/conda/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 107, in _train_coordinator_fn\n    trainer.training_loop()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/data_parallel_trainer.py\", line 459, in training_loop\n    backend_executor.start()\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/backend_executor.py\", line 203, in start\n    self._backend.on_start(self.worker_group, self._backend_config)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/torch/config.py\", line 200, in on_start\n    ray.get(setup_futures)\nray.exceptions.RayTaskError(DistNetworkError): \u001b[36mray::_RayTrainWorker__execute._setup_torch_process_group()\u001b[39m (pid=456, ip=100.64.71.249, actor_id=59667c756cd57fb591cdc7bf0e000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f50b0fe6680>)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 30, in __execute\n    return func(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/train/torch/config.py\", line 115, in _setup_torch_process_group\n    dist.init_process_group(\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 89, in wrapper\n    func_return = func(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 1305, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 246, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 174, in _create_c10d_store\n    return TCPStore(\ntorch.distributed.DistNetworkError: Connection reset by peer\nException raised from recvBytes at ../torch/csrc/distributed/c10d/Utils.hpp:669 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f50b00a9897 in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x5b39ecc (0x7f508a1bfecc in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: c10d::TCPStore::incrementValueBy(std::string const&, long) + 0x155 (0x7f508a1bae65 in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: c10d::TCPStore::waitForWorkers() + 0xbb (0x7f508a1bdb6b in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: c10d::TCPStore::TCPStore(std::string, c10d::TCPStoreOptions const&) + 0x64c (0x7f508a1be3ec in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xce1aad (0x7f50aa3dcaad in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\nframe #6: <unknown function> + 0x47def4 (0x7f50a9b78ef4 in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\nframe #7: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x4fdc87]\nframe #8: _PyObject_MakeTpCall + 0x25b (0x4f741b in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #9: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x509cbf]\nframe #10: PyVectorcall_Call + 0xb9 (0x50a869 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #11: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x507a1c]\nframe #12: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x4f7786]\nframe #13: <unknown function> + 0x47c74b (0x7f50a9b7774b in /home/ray/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\nframe #14: _PyObject_MakeTpCall + 0x25b (0x4f741b in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #15: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #16: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #17: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #18: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x571c97]\nframe #19: ray::_RayTrainWorker__execute._setup_torch_process_group() [0x4fe2b4]\nframe #20: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #21: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #22: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #23: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #25: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #26: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #27: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #28: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #29: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #30: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #33: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #34: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #35: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #36: PyObject_Call + 0xb8 (0x50a508 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #39: PyVectorcall_Call + 0xb9 (0x50a869 in ray::_RayTrainWorker__execute._setup_torch_process_group)\nframe #40: <unknown function> + 0x64b9cf (0x7f525d5459cf in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #41: <unknown function> + 0x6a2621 (0x7f525d59c621 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #42: <unknown function> + 0x64b9cf (0x7f525d5459cf in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #43: <unknown function> + 0x734c16 (0x7f525d62ec16 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #44: std::_Function_handler<ray::Status (ray::rpc::Address const&, ray::rpc::TaskType, std::string, ray::core::RayFunction const&, std::unordered_map<std::string, double, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, double> > > const&, std::vector<std::shared_ptr<ray::RayObject>, std::allocator<std::shared_ptr<ray::RayObject> > > const&, std::vector<ray::rpc::ObjectReference, std::allocator<ray::rpc::ObjectReference> > const&, std::string const&, std::string const&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, std::shared_ptr<ray::LocalMemoryBuffer>&, bool*, std::string*, std::vector<ray::ConcurrencyGroup, std::allocator<ray::ConcurrencyGroup> > const&, std::string, bool, bool, bool, long), ray::Status (*)(ray::rpc::Address const&, ray::rpc::TaskType, std::string, ray::core::RayFunction const&, std::unordered_map<std::string, double, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, double> > > const&, std::vector<std::shared_ptr<ray::RayObject>, std::allocator<std::shared_ptr<ray::RayObject> > > const&, std::vector<ray::rpc::ObjectReference, std::allocator<ray::rpc::ObjectReference> > const&, std::string, std::string, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, std::shared_ptr<ray::LocalMemoryBuffer>&, bool*, std::string*, std::vector<ray::ConcurrencyGroup, std::allocator<ray::ConcurrencyGroup> > const&, std::string, bool, bool, bool, long)>::_M_invoke(std::_Any_data const&, ray::rpc::Address const&, ray::rpc::TaskType&&, std::string&&, ray::core::RayFunction const&, std::unordered_map<std::string, double, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, double> > > const&, std::vector<std::shared_ptr<ray::RayObject>, std::allocator<std::shared_ptr<ray::RayObject> > > const&, std::vector<ray::rpc::ObjectReference, std::allocator<ray::rpc::ObjectReference> > const&, std::string const&, std::string const&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*&&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*&&, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*&&, std::shared_ptr<ray::LocalMemoryBuffer>&, bool*&&, std::string*&&, std::vector<ray::ConcurrencyGroup, std::allocator<ray::ConcurrencyGroup> > const&, std::string&&, bool&&, bool&&, bool&&, long&&) + 0x169 (0x7f525d54baa9 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #45: ray::core::CoreWorker::ExecuteTask(ray::TaskSpecification const&, std::shared_ptr<std::unordered_map<std::string, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > > > > > > const&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, google::protobuf::RepeatedPtrField<ray::rpc::ObjectReferenceCount>*, bool*, std::string*) + 0xd50 (0x7f525d763610 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #46: std::_Function_handler<ray::Status (ray::TaskSpecification const&, std::shared_ptr<std::unordered_map<std::string, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > > > > > >, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, google::protobuf::RepeatedPtrField<ray::rpc::ObjectReferenceCount>*, bool*, std::string*), std::_Bind<ray::Status (ray::core::CoreWorker::*(ray::core::CoreWorker*, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>, std::_Placeholder<6>, std::_Placeholder<7>, std::_Placeholder<8>))(ray::TaskSpecification const&, std::shared_ptr<std::unordered_map<std::string, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > > > > > > const&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*, google::protobuf::RepeatedPtrField<ray::rpc::ObjectReferenceCount>*, bool*, std::string*)> >::_M_invoke(std::_Any_data const&, ray::TaskSpecification const&, std::shared_ptr<std::unordered_map<std::string, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<std::pair<long, double>, std::allocator<std::pair<long, double> > > > > > >&&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*&&, std::vector<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> >, std::allocator<std::pair<ray::ObjectID, std::shared_ptr<ray::RayObject> > > >*&&, std::vector<std::pair<ray::ObjectID, bool>, std::allocator<std::pair<ray::ObjectID, bool> > >*&&, google::protobuf::RepeatedPtrField<ray::rpc::ObjectReferenceCount>*&&, bool*&&, std::string*&&) + 0x58 (0x7f525d68db98 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #47: <unknown function> + 0x8718c4 (0x7f525d76b8c4 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #48: <unknown function> + 0x872cba (0x7f525d76ccba in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #49: <unknown function> + 0x8ab91e (0x7f525d7a591e in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #50: ray::core::ActorSchedulingQueue::AcceptRequestOrRejectIfCanceled(ray::TaskID, ray::core::InboundRequest&) + 0x114 (0x7f525d7a6994 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #51: <unknown function> + 0x8b0673 (0x7f525d7aa673 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #52: ray::core::ActorSchedulingQueue::Add(long, long, std::function<void (std::function<void (ray::Status, std::function<void ()>, std::function<void ()>)>)>, std::function<void (ray::Status const&, std::function<void (ray::Status, std::function<void ()>, std::function<void ()>)>)>, std::function<void (ray::Status, std::function<void ()>, std::function<void ()>)>, std::string const&, std::shared_ptr<ray::FunctionDescriptorInterface> const&, ray::TaskID, std::vector<ray::rpc::ObjectReference, std::allocator<ray::rpc::ObjectReference> > const&) + 0x4b3 (0x7f525d7a9083 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #53: ray::core::TaskReceiver::HandleTask(ray::rpc::PushTaskRequest const&, ray::rpc::PushTaskReply*, std::function<void (ray::Status, std::function<void ()>, std::function<void ()>)>) + 0x1137 (0x7f525d76e347 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #54: <unknown function> + 0x7b097d (0x7f525d6aa97d in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #55: <unknown function> + 0xb3ea1c (0x7f525da38a1c in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #56: <unknown function> + 0xb3a5fe (0x7f525da345fe in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #57: <unknown function> + 0xb3aa76 (0x7f525da34a76 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #58: <unknown function> + 0x110c59b (0x7f525e00659b in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #59: <unknown function> + 0x110df19 (0x7f525e007f19 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #60: <unknown function> + 0x110e622 (0x7f525e008622 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #61: ray::core::CoreWorker::RunTaskExecutionLoop() + 0xcd (0x7f525d6a6a8d in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #62: ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop() + 0x85 (0x7f525d770315 in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)\nframe #63: ray::core::CoreWorkerProcess::RunTaskExecutionLoop() + 0x1d (0x7f525d77051d in /home/ray/anaconda3/lib/python3.10/site-packages/ray/_raylet.so)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTrainingFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 113\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTUNE_DISABLE_AUTO_CALLBACKS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TorchTrainer(\n\u001b[1;32m     97\u001b[0m     train_loop_per_worker,\n\u001b[1;32m     98\u001b[0m     train_loop_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     ),\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 113\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ray/train/base_trainer.py:638\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m result \u001b[38;5;241m=\u001b[39m result_grid[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([restore_msg, TrainingFailedError\u001b[38;5;241m.\u001b[39m_FAILURE_CONFIG_MSG])\n\u001b[1;32m    640\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresult\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTrainingFailedError\u001b[0m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/mnt/data/ddl-end-to-end-demo/unknown_job/ray_results/mnist_torch_ddp_0e000000\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m Wrote the latest version of all result files and experiment state to '/mnt/data/ddl-end-to-end-demo/unknown_job/ray_results/mnist_torch_ddp_0e000000' in 0.0480s.\n",
      "\u001b[36m(TunerInternal pid=7637)\u001b[0m Trials did not complete: [TorchTrainer_42653_00000]\n"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43ea18d3-8956-48b7-84fe-5efab74508dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45367d66-4f56-4e1a-98b0-44504bd70657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
